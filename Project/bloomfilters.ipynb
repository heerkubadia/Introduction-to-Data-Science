{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATASET AND HELPER FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBVv68T6Y2jH",
        "outputId": "dd5d344b-841e-4d14-96df-362764c3057b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original classes: ['phishing' 'benign' 'defacement' 'malware']\n",
            "label\n",
            "safe      428103\n",
            "unsafe    223088\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv(\"/content/malicious_phish.csv\")\n",
        "\n",
        "# Print unique classes to verify\n",
        "print(\"Original classes:\", df['type'].unique())\n",
        "\n",
        "# Convert multi-class labels to binary\n",
        "df['label'] = df['type'].apply(lambda x: 'safe' if x.lower() == 'benign' else 'unsafe')\n",
        "\n",
        "# Drop the old column if you no longer need it\n",
        "df.drop(columns=['type'], inplace=True)\n",
        "\n",
        "# Save new version\n",
        "df.to_csv(\"binary_url_safe_urls_train.csv\", index=False)\n",
        "\n",
        "# Check new label distribution\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pybloom_live"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pybloom_live import BloomFilter, ScalableBloomFilter\n",
        "import time\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import psutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# PyTorch imports for GPU support\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class PyTorchLogisticRegression(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of Logistic Regression that can use GPU if available.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(PyTorchLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchRandomForest(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of a neural network that approximates a Random Forest.\n",
        "    This is a simplified version with more hidden units to capture complex patterns.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(PyTorchRandomForest, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_url(url):\n",
        "    \"\"\"Extract features from URL for ML model.\"\"\"\n",
        "    # Basic preprocessing - convert to lowercase and remove punctuation\n",
        "    url = url.lower()\n",
        "    url = re.sub(r'[^\\w\\s]', '', url)\n",
        "\n",
        "    # Extract simple features\n",
        "    features = {\n",
        "        'length': len(url),\n",
        "        'num_digits': sum(c.isdigit() for c in url),\n",
        "        'num_segments': len(url.split('/')),\n",
        "    }\n",
        "\n",
        "    return url, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STANDARD BLOOM FILTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_standard_bloom_filter(train_urls, test_urls, capacity, error_rate):\n",
        "    \"\"\"\n",
        "    Evaluate a standard Bloom filter.\n",
        "\n",
        "    Args:\n",
        "        train_urls: URLs to add to the filter\n",
        "        test_urls: URLs to test against the filter\n",
        "        capacity: Expected number of elements\n",
        "        error_rate: Desired false positive rate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating Standard Bloom Filter with error_rate={error_rate}\")\n",
        "\n",
        "    # Initialize memory and time tracking\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Calculate the size of the Bloom filter in bits\n",
        "    filter_bits = int(-capacity * np.log(error_rate) / (np.log(2) ** 2))\n",
        "    filter_size_mb = filter_bits / 8 / 1024 \n",
        "    print(f\"Calculated Bloom filter size: {filter_size_mb:.2f} KB ({filter_bits} bits)\")\n",
        "\n",
        "    # Create Bloom filter\n",
        "    bloom = BloomFilter(capacity=capacity, error_rate=error_rate)\n",
        "\n",
        "    creation_time = time.time() - start_time\n",
        "\n",
        "    # Add all training URLs to the filter\n",
        "    print(f\"Adding {len(train_urls)} URLs to the filter...\")\n",
        "    add_start_time = time.time()\n",
        "    progress_bar = tqdm(total=len(train_urls), desc=\"Adding URLs\", unit=\"url\")\n",
        "\n",
        "    for url in train_urls[\"url\"]:\n",
        "        bloom.add(url)\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    add_time = time.time() - add_start_time\n",
        "    print(f\"Added {len(train_urls)} URLs in {add_time:.2f} seconds\")\n",
        "\n",
        "    # Test on the test_urls\n",
        "    print(f\"Testing {len(test_urls)} URLs against the filter...\")\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    membership_check = list(train_urls['url'])\n",
        "\n",
        "    progress_bar = tqdm(total=len(test_urls), desc=\"Testing URLs\", unit=\"url\")\n",
        "    for url in test_urls[\"url\"]:\n",
        "        if url in bloom:\n",
        "            if url in membership_check:\n",
        "                true_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "        else:\n",
        "            if url in membership_check:\n",
        "                false_negatives += 1\n",
        "            else:\n",
        "                true_negatives += 1\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Tested {len(test_urls)} URLs in {test_time:.2f} seconds\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    false_positive_rate = false_positives / len(test_urls) if len(test_urls) > 0 else 0\n",
        "    false_negative_rate = false_negatives / len(train_urls) if len(train_urls) > 0 else 0\n",
        "    bits_per_item = filter_bits / len(train_urls) if len(train_urls) > 0 else 0\n",
        "\n",
        "    # Prepare results\n",
        "    results = {\n",
        "        \"filter_type\": f\"Standard Bloom Filter (error_rate={error_rate})\",\n",
        "        \"error_rate\": error_rate,\n",
        "        \"capacity\": capacity,\n",
        "        \"filter_size_bits\": filter_bits,\n",
        "        \"filter_size_kb\": filter_size_mb,\n",
        "        \"bits_per_item\": bits_per_item,\n",
        "        \"creation_time\": creation_time,\n",
        "        \"add_time\": add_time,\n",
        "        \"true_positives\": true_positives,\n",
        "        \"false_negatives\": false_negatives,\n",
        "        \"true_negatives\": true_negatives,\n",
        "        \"false_positives\": false_positives,\n",
        "        \"false_positive_rate\": false_positive_rate,\n",
        "        \"false_negative_rate\": false_negative_rate,\n",
        "        \"test_time\": test_time,\n",
        "        \"accuracy\": (true_positives + true_negatives) / len(test_urls) if len(test_urls) > 0 else 0,\n",
        "        \"precision\": true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
        "        \"recall\": true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
        "        \"f1_score\": (2 * true_positives) / (2 * true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"  Filter type: {results['filter_type']}\")\n",
        "    # print(f\"  Capacity: {results['capacity']}\")\n",
        "    print(f\"  Filter size: {results['filter_size_kb']:.4f} KB\")\n",
        "    print(f\"  Bits per item: {results['bits_per_item']:.2f}\")\n",
        "    # print(f\"  Creation time: {results['creation_time']:.2f} seconds\")\n",
        "    # print(f\"  Add time: {results['add_time']:.2f} seconds\")\n",
        "    print(f\"  Query time: {results['test_time']:.2f} seconds\")\n",
        "    print(f\"  True positives: {results['true_positives']}\")\n",
        "    print(f\"  False positives: {results['false_positives']}\")\n",
        "    print(f\"  True negatives: {results['true_negatives']}\")\n",
        "    print(f\"  False negatives: {results['false_negatives']}\")\n",
        "    print(f\"  False positive rate: {results['false_positive_rate']:.4f} (expected: {results['error_rate']})\")\n",
        "    print(f\"  False negative rate: {results['false_negative_rate']:.4f}\")\n",
        "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PARTITIONED LEARNED BLOOM FILTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PartitionedLearnedBloomFilter:\n",
        "    \"\"\"\n",
        "    Partitioned Learned Bloom Filter implementation.\n",
        "\n",
        "    This filter uses a machine learning model to partition the input space\n",
        "    and assigns different Bloom filters to different partitions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity, error_rate=0.01, num_partitions=3):\n",
        "        \"\"\"\n",
        "        Initialize the Partitioned Learned Bloom Filter.\n",
        "\n",
        "        Args:\n",
        "            capacity: Expected number of elements\n",
        "            error_rate: Desired false positive rate\n",
        "            num_partitions: Number of partitions to create\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.error_rate = error_rate\n",
        "        self.num_partitions = num_partitions\n",
        "\n",
        "        # Check if CUDA is available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize ML model\n",
        "        self.model = None\n",
        "        self.pytorch_model = None\n",
        "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
        "\n",
        "        # Thresholds for partitioning\n",
        "        self.thresholds = []\n",
        "\n",
        "        # Initialize Bloom filters for each partition\n",
        "        # We'll adjust capacities based on expected elements per partition\n",
        "        self.bloom_filters = []\n",
        "        self.partition_capacities = []\n",
        "\n",
        "        # Statistics\n",
        "        self.items_added = 0\n",
        "        self.items_per_partition = [0] * num_partitions\n",
        "\n",
        "    def train_model(self, train_urls, train_labels):\n",
        "        \"\"\"\n",
        "        Train the ML model for partitioning.\n",
        "\n",
        "        Args:\n",
        "            train_urls: List of URLs for training\n",
        "            train_labels: Binary labels (0 for safe, 1 for unsafe)\n",
        "        \"\"\"\n",
        "        print(\"Training ML model for partitioning...\")\n",
        "\n",
        "        # First, fit the vectorizer to transform text data\n",
        "        print(\"Fitting vectorizer...\")\n",
        "        X = self.vectorizer.fit_transform(train_urls)\n",
        "\n",
        "        # Convert labels to numpy array if they're not already\n",
        "        train_labels = np.array(train_labels)\n",
        "\n",
        "        # For CPU fallback, use scikit-learn pipeline\n",
        "        if self.device.type == 'cpu':\n",
        "            print(\"Using CPU with scikit-learn pipeline\")\n",
        "            self.model = Pipeline([\n",
        "                ('classifier', LogisticRegression(max_iter=1000))\n",
        "            ])\n",
        "\n",
        "            print(\"Fitting model to training data...\")\n",
        "            self.model.fit(X, train_labels)\n",
        "\n",
        "            # Get predicted probabilities\n",
        "            print(\"Getting predicted probabilities...\")\n",
        "            # Create a progress bar for prediction\n",
        "            probs = np.zeros(len(train_urls))\n",
        "            progress_bar = tqdm(total=len(train_urls), desc=\"Predicting probabilities\", unit=\"url\")\n",
        "\n",
        "            # Process in batches to show progress\n",
        "            batch_size = 1000\n",
        "            for i in range(0, len(train_urls), batch_size):\n",
        "                batch_end = min(i + batch_size, len(train_urls))\n",
        "                batch_vectors = X[i:batch_end]\n",
        "                batch_probs = self.model.predict_proba(batch_vectors)[:, 1]\n",
        "                probs[i:batch_end] = batch_probs\n",
        "                progress_bar.update((batch_vectors.shape[0]))\n",
        "\n",
        "            progress_bar.close()\n",
        "        else:\n",
        "            # For GPU, use PyTorch model\n",
        "            print(\"Using GPU with PyTorch model\")\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            X_tensor = torch.FloatTensor(X.toarray()).to(self.device)\n",
        "            y_tensor = torch.FloatTensor(train_labels.reshape(-1, 1)).to(self.device)\n",
        "\n",
        "            # Create PyTorch model\n",
        "            input_dim = X.shape[1]\n",
        "            self.pytorch_model = PyTorchLogisticRegression(input_dim).to(self.device)\n",
        "\n",
        "            # Define loss function and optimizer\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.Adam(self.pytorch_model.parameters(), lr=0.01)\n",
        "\n",
        "            # Create DataLoader for batch processing\n",
        "            dataset = TensorDataset(X_tensor, y_tensor)\n",
        "            dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "            # Train the model\n",
        "            print(\"Training PyTorch model...\")\n",
        "            num_epochs = 10\n",
        "            for epoch in range(num_epochs):\n",
        "                running_loss = 0.0\n",
        "                progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "\n",
        "                for inputs, labels in progress_bar:\n",
        "                    # Zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.pytorch_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass and optimize\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Update statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "                epoch_loss = running_loss / len(dataset)\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            self.pytorch_model.eval()\n",
        "\n",
        "            # Get predicted probabilities\n",
        "            print(\"Getting predicted probabilities...\")\n",
        "            unsafe_train_urls = [url for url, label in zip(train_urls, train_labels) if label == 1]\n",
        "\n",
        "            probs = np.zeros(len(unsafe_train_urls))\n",
        "            progress_bar = tqdm(total=len(unsafe_train_urls), desc=\"Predicting probabilities\", unit=\"url\")\n",
        "\n",
        "            # Process in batches to show progress\n",
        "            batch_size = 1000\n",
        "            with torch.no_grad():\n",
        "                for i in range(0, len(unsafe_train_urls), batch_size):\n",
        "                    batch_end = min(i + batch_size, len(unsafe_train_urls))\n",
        "                    batch_vectors = X[i:batch_end].toarray()\n",
        "                    batch_tensor = torch.FloatTensor(batch_vectors).to(self.device)\n",
        "                    batch_probs = self.pytorch_model(batch_tensor).cpu().numpy().flatten()\n",
        "                    probs[i:batch_end] = batch_probs\n",
        "                    progress_bar.update((batch_vectors.shape[0]))\n",
        "\n",
        "            progress_bar.close()\n",
        "\n",
        "        # Compute thresholds for partitioning\n",
        "        self.thresholds = []\n",
        "        for i in range(1, self.num_partitions):\n",
        "            threshold = np.percentile(probs, (i * 100) / self.num_partitions)\n",
        "            self.thresholds.append(threshold)\n",
        "\n",
        "        # Count expected elements per partition\n",
        "        partition_counts = [0] * self.num_partitions\n",
        "        progress_bar = tqdm(total=len(probs), desc=\"Calculating partition counts\", unit=\"item\")\n",
        "\n",
        "        # Process in batches for better performance\n",
        "        batch_size = 10000\n",
        "        for i in range(0, len(probs), batch_size):\n",
        "            batch_end = min(i + batch_size, len(probs))\n",
        "            for prob in probs[i:batch_end]:\n",
        "                partition = self._get_partition(prob)\n",
        "                partition_counts[partition] += 1\n",
        "            progress_bar.update(batch_end - i)\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Calculate capacity for each partition\n",
        "        # Add some buffer to each partition to avoid 100% fill rates\n",
        "        total_size = 0\n",
        "        total_count = sum(partition_counts)\n",
        "        self.partition_capacities = [\n",
        "            max(int((count / total_count) * self.capacity * 1.2), 1)  # Add 20% buffer\n",
        "            for count in partition_counts\n",
        "        ]\n",
        "\n",
        "        # Initialize ScalableBloomFilters with adjusted initial capacities\n",
        "        self.bloom_filters = [\n",
        "            ScalableBloomFilter(initial_capacity=cap, error_rate=self.error_rate)\n",
        "            for cap in self.partition_capacities\n",
        "        ]\n",
        "\n",
        "        print(f\"Model trained. Partition capacities: {self.partition_capacities}\")\n",
        "\n",
        "    def _get_partition(self, probability):\n",
        "        \"\"\"\n",
        "        Determine which partition an element belongs to based on its probability.\n",
        "\n",
        "        Args:\n",
        "            probability: ML model's predicted probability\n",
        "\n",
        "        Returns:\n",
        "            Partition index (0 to num_partitions-1)\n",
        "        \"\"\"\n",
        "        for i, threshold in enumerate(self.thresholds):\n",
        "            if probability < threshold:\n",
        "                return i\n",
        "        return self.num_partitions - 1\n",
        "\n",
        "    def _get_probability(self, url):\n",
        "        \"\"\"\n",
        "        Get the probability from the model for a single URL.\n",
        "\n",
        "        Args:\n",
        "            url: URL to get probability for\n",
        "\n",
        "        Returns:\n",
        "            Probability value\n",
        "        \"\"\"\n",
        "        if self.device.type == 'cpu' or self.pytorch_model is None:\n",
        "            # Use scikit-learn model\n",
        "            url_vector = self.vectorizer.transform([url])\n",
        "            return self.model.predict_proba(url_vector)[0, 1]\n",
        "        else:\n",
        "            # Use PyTorch model\n",
        "            url_vector = self.vectorizer.transform([url]).toarray()\n",
        "            url_tensor = torch.FloatTensor(url_vector).to(self.device)\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                prob = self.pytorch_model(url_tensor).cpu().numpy()[0, 0]\n",
        "\n",
        "            return prob\n",
        "\n",
        "    def add(self, url):\n",
        "        \"\"\"\n",
        "        Add a URL to the filter.\n",
        "\n",
        "        Args:\n",
        "            url: URL to add\n",
        "        \"\"\"\n",
        "        if (self.device.type == 'cpu' and self.model is None) or \\\n",
        "           (self.device.type == 'cuda' and self.pytorch_model is None):\n",
        "            raise ValueError(\"Model must be trained before adding elements\")\n",
        "\n",
        "        # Get probability from model\n",
        "        prob = self._get_probability(url)\n",
        "\n",
        "        # Determine partition\n",
        "        partition = self._get_partition(prob)\n",
        "\n",
        "        # Add to corresponding Bloom filter\n",
        "        self.bloom_filters[partition].add(url)\n",
        "\n",
        "        # Update statistics\n",
        "        self.items_added += 1\n",
        "        self.items_per_partition[partition] += 1\n",
        "\n",
        "    def query(self, url):\n",
        "        \"\"\"\n",
        "        Query if a URL is in the filter.\n",
        "\n",
        "        Args:\n",
        "            url: URL to query\n",
        "\n",
        "        Returns:\n",
        "            True if the URL might be in the filter, False if definitely not\n",
        "        \"\"\"\n",
        "        if (self.device.type == 'cpu' and self.model is None) or \\\n",
        "           (self.device.type == 'cuda' and self.pytorch_model is None):\n",
        "            raise ValueError(\"Model must be trained before querying\")\n",
        "\n",
        "        # Get probability from model\n",
        "        prob = self._get_probability(url)\n",
        "\n",
        "        # Determine partition\n",
        "        partition = self._get_partition(prob)\n",
        "\n",
        "        # Query corresponding Bloom filter\n",
        "        return url in self.bloom_filters[partition]\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Return statistics about the filter.\"\"\"\n",
        "        # Calculate total bits used by all Bloom filters\n",
        "        total_bits = 0\n",
        "        bloom_filter_sizes_mb = []\n",
        "\n",
        "        for bf in self.bloom_filters:\n",
        "            # ScalableBloomFilter doesn't have a bitarray attribute\n",
        "            # Instead, it has a series of BloomFilter objects in its 'filters' attribute\n",
        "            if hasattr(bf, 'filters'):\n",
        "                # For ScalableBloomFilter\n",
        "                bf_bits = sum(len(f.bitarray) for f in bf.filters)\n",
        "                bf_size_mb = bf_bits / 8 / 1024 / 1024  # Convert to MB\n",
        "            else:\n",
        "                # For regular BloomFilter (fallback)\n",
        "                bf_bits = len(bf.bitarray)\n",
        "                bf_size_mb = bf_bits / 8 / 1024 / 1024  # Convert to MB\n",
        "\n",
        "            total_bits += bf_bits\n",
        "            bloom_filter_sizes_mb.append(bf_size_mb)\n",
        "\n",
        "        total_size_mb = total_bits / 8 / 1024 / 1024  # Convert bits to MB\n",
        "\n",
        "        # Calculate bits per item\n",
        "        bits_per_item = total_bits / self.items_added if self.items_added > 0 else 0\n",
        "\n",
        "        stats = {\n",
        "            \"total_items\": self.items_added,\n",
        "            \"items_per_partition\": self.items_per_partition,\n",
        "            \"partition_capacities\": self.partition_capacities,\n",
        "            \"partition_fill_rates\": [\n",
        "                self.items_per_partition[i] / self.partition_capacities[i]\n",
        "                for i in range(self.num_partitions)\n",
        "            ],\n",
        "            \"num_partitions\": self.num_partitions,\n",
        "            \"error_rate\": self.error_rate,\n",
        "            \"total_bits\": total_bits,\n",
        "            \"total_size_mb\": total_size_mb,\n",
        "            \"bits_per_item\": bits_per_item,\n",
        "            \"bloom_filter_sizes_mb\": bloom_filter_sizes_mb\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Return the current memory usage in MB.\"\"\"\n",
        "    # Force garbage collection before measuring memory\n",
        "    gc.collect()\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 * 1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_partitioned_learned_bloom_filter(train_urls, train_labels, test_urls, test_labels, capacity, error_rate, num_partitions):\n",
        "    \"\"\"\n",
        "    Evaluate a Partitioned Learned Bloom Filter.\n",
        "\n",
        "    Args:\n",
        "        train_urls: URLs for training\n",
        "        train_labels: Labels for training (0 for safe, 1 for unsafe)\n",
        "        test_urls: URLs for testing\n",
        "        test_labels: Labels for testing\n",
        "        capacity: Expected number of elements\n",
        "        error_rate: Desired false positive rate\n",
        "        num_partitions: Number of partitions\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating Partitioned Learned Bloom Filter with {num_partitions} partitions and error_rate={error_rate}\")\n",
        "\n",
        "    # Initialize memory and time tracking\n",
        "    start_time = time.time()\n",
        "    memory_before = get_memory_usage()\n",
        "    print(f\"Initial memory usage: {memory_before:.2f} MB\")\n",
        "\n",
        "    # Create and train PLBF\n",
        "    plbf = PartitionedLearnedBloomFilter(capacity, error_rate, num_partitions)\n",
        "    plbf.train_model(train_urls, train_labels)\n",
        "\n",
        "    # Track memory after model creation\n",
        "    memory_after_model = get_memory_usage()\n",
        "    print(f\"Memory after model creation: {memory_after_model:.2f} MB\")\n",
        "    model_creation_time = time.time() - start_time\n",
        "    unsafe_train_urls = [url for url, label in zip(train_urls, train_labels) if label == 1]\n",
        "\n",
        "    # Add all training URLs to the filter\n",
        "    print(f\"Adding {len(unsafe_train_urls)} URLs to the filter...\")\n",
        "    add_start_time = time.time()\n",
        "    progress_bar = tqdm(total=len(unsafe_train_urls), desc=\"Adding URLs\", unit=\"url\")\n",
        "    \n",
        "    # Process in batches for better progress tracking\n",
        "    batch_size = 1000\n",
        "    for i in range(0, len(unsafe_train_urls), batch_size):\n",
        "        batch_end = min(i + batch_size, len(unsafe_train_urls))\n",
        "        for url in unsafe_train_urls[i:batch_end]:\n",
        "            plbf.add(url)\n",
        "        progress_bar.update(batch_end - i)\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    add_time = time.time() - add_start_time\n",
        "    memory_after_add = get_memory_usage()\n",
        "    print(f\"Memory after adding URLs: {memory_after_add:.2f} MB\")\n",
        "    # Test on the test_urls\n",
        "    print(f\"Testing {len(test_urls)} URLs against the filter...\")\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    progress_bar = tqdm(total=len(test_urls), desc=\"Testing URLs\", unit=\"url\")\n",
        "    for url, label in zip(test_urls, test_labels):\n",
        "        result = plbf.query(url)  # Predicts: unsafe (True) or safe (False)\n",
        "\n",
        "        if result:  # Bloom filter says: possibly unsafe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                true_positives += 1\n",
        "            else:           # Actually safe\n",
        "                false_positives += 1\n",
        "        else:  # Bloom filter says: definitely safe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                false_negatives += 1\n",
        "            else:           # Actually safe\n",
        "                true_negatives += 1\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Tested {len(test_urls)} URLs in {test_time:.2f} seconds\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    false_positive_rate = false_positives / len(test_urls) if len(test_urls) > 0 else 0\n",
        "    false_negative_rate = false_negatives / len(train_urls) if len(train_urls) > 0 else 0\n",
        "    # bits_per_item = filter_bits / len(train_urls) if len(train_urls) > 0 else 0\n",
        "\n",
        "    # Get filter stats\n",
        "    filter_stats = plbf.get_stats()\n",
        "\n",
        "    # Prepare results\n",
        "    results = {\n",
        "        \"filter_type\": f\"PLBF (partitions={num_partitions}, error_rate={error_rate})\",\n",
        "        \"num_partitions\": num_partitions,\n",
        "        \"error_rate\": error_rate,\n",
        "        \"capacity\": capacity,\n",
        "        \"memory_before\": memory_before,\n",
        "        \"memory_after_model\": memory_after_model,\n",
        "        \"memory_after_add\": memory_after_add,\n",
        "        \"memory_for_model\": memory_after_model - memory_before,\n",
        "        \"memory_for_filters\": filter_stats[\"total_size_mb\"],\n",
        "        \"total_memory\": (memory_after_model - memory_before) + filter_stats[\"total_size_mb\"],\n",
        "        \"model_creation_time\": model_creation_time,\n",
        "        \"add_time\": add_time,\n",
        "        \"items_per_partition\": filter_stats[\"items_per_partition\"],\n",
        "        \"partition_fill_rates\": filter_stats[\"partition_fill_rates\"],\n",
        "        \"bits_per_item\": filter_stats[\"bits_per_item\"],\n",
        "        \"total_bits\": filter_stats[\"total_bits\"],\n",
        "        \"capacity\": capacity,\n",
        "        \"true_positives\": true_positives,\n",
        "        \"false_negatives\": false_negatives,\n",
        "        \"true_negatives\": true_negatives,\n",
        "        \"false_positives\": false_positives,\n",
        "        \"false_positive_rate\": false_positive_rate,\n",
        "        \"false_negative_rate\": false_negative_rate,\n",
        "        \"test_time\": test_time,\n",
        "        \"accuracy\": (true_positives + true_negatives) / len(test_urls) if len(test_urls) > 0 else 0,\n",
        "        \"precision\": true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
        "        \"recall\": true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
        "        \"f1_score\": (2 * true_positives) / (2 * true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"  Filter type: {results['filter_type']}\")\n",
        "   \n",
        "    print(f\"  Memory for model: {results['memory_for_model']:.2f} MB\")\n",
        "    print(f\"  Memory for Bloom filters: {results['memory_for_filters']:.2f} MB\")\n",
        "    print(f\"  Total memory (model + filters): {results['total_memory']:.2f} MB\")\n",
        "    print(f\"  Bits per item stats: {results['bits_per_item']:.2f}\")\n",
        "    # print(f\"  Creation time: {results['creation_time']:.2f} seconds\")\n",
        "    # print(f\"  Add time: {results['add_time']:.2f} seconds\")\n",
        "    print(f\"  Query time: {results['test_time']:.2f} seconds\")\n",
        "    print(f\"  Items per partition: {filter_stats['items_per_partition']}\")\n",
        "    print(f\"  Partition fill rates: {[f'{rate:.2f}' for rate in filter_stats['partition_fill_rates']]}\")\n",
        "\n",
        "    print(f\"  True positives: {results['true_positives']}\")\n",
        "    print(f\"  False positives: {results['false_positives']}\")\n",
        "    print(f\"  True negatives: {results['true_negatives']}\")\n",
        "    print(f\"  False negatives: {results['false_negatives']}\")\n",
        "    print(f\"  False positive rate: {results['false_positive_rate']:.4f} (expected: {results['error_rate']})\")\n",
        "    print(f\"  False negative rate: {results['false_negative_rate']:.4f}\")\n",
        "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
        "\n",
        "    return results\n",
        "    \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CASCADED LEARNED BLOOM FILTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CascadedLearnedBloomFilter:\n",
        "    \"\"\"\n",
        "    Cascaded Learned Bloom Filter implementation.\n",
        "\n",
        "    This filter uses a cascade of machine learning models and Bloom filters\n",
        "    to achieve optimal model-filter size balance and fast rejection.\n",
        "    The final stage includes a safety net Bloom filter to catch any false negatives.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity, error_rate=0.01, num_stages=2, thresholds=None):\n",
        "        \"\"\"\n",
        "        Initialize the Cascaded Learned Bloom Filter.\n",
        "\n",
        "        Args:\n",
        "            capacity: Expected number of elements\n",
        "            error_rate: Desired false positive rate\n",
        "            num_stages: Number of cascade stages\n",
        "            thresholds: List of thresholds for each stage (if None, will be determined automatically)\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.error_rate = error_rate\n",
        "        self.num_stages = num_stages\n",
        "\n",
        "        # Check if CUDA is available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize ML models for each stage\n",
        "        self.models = []\n",
        "        self.pytorch_models = []\n",
        "        self.vectorizers = []\n",
        "        for _ in range(num_stages):\n",
        "            self.vectorizers.append(TfidfVectorizer(analyzer='char', ngram_range=(3, 5)))\n",
        "            self.models.append(None)\n",
        "            self.pytorch_models.append(None)\n",
        "\n",
        "        # Thresholds for each stage\n",
        "        if thresholds is None:\n",
        "            # Default thresholds will be determined during training\n",
        "            self.thresholds = [0.5] * num_stages\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "\n",
        "        # Initialize Bloom filters for each stage\n",
        "        self.bloom_filters = []\n",
        "\n",
        "        # Statistics\n",
        "        self.items_added = 0\n",
        "        self.stage_stats = [{\n",
        "            'items_processed': 0,\n",
        "            'items_rejected': 0,\n",
        "            'items_passed': 0\n",
        "        } for _ in range(num_stages)]\n",
        "        \n",
        "        # Track false negatives for final safety net Bloom filter\n",
        "        self.false_negatives_detected = 0\n",
        "\n",
        "    def _create_model(self, stage):\n",
        "        \"\"\"Create a model for a specific stage.\"\"\"\n",
        "        if self.device.type == 'cpu':\n",
        "            # CPU-based models using scikit-learn\n",
        "            if stage == 0:\n",
        "                # First stage uses a simpler model for faster processing\n",
        "                return Pipeline([\n",
        "                    ('vectorizer', self.vectorizers[stage]),\n",
        "                    ('classifier', LogisticRegression(max_iter=1000))\n",
        "                ])\n",
        "            else:\n",
        "                # Later stages use more complex models for better accuracy\n",
        "                return Pipeline([\n",
        "                    ('vectorizer', self.vectorizers[stage]),\n",
        "                    ('classifier', RandomForestClassifier(n_estimators=100))\n",
        "                ])\n",
        "        else:\n",
        "            # GPU-based models using PyTorch\n",
        "            # We'll create the actual PyTorch models during training\n",
        "            # This method just returns a placeholder for scikit-learn compatibility\n",
        "            if stage == 0:\n",
        "                # First stage uses a simpler model for faster processing\n",
        "                return Pipeline([\n",
        "                    ('vectorizer', self.vectorizers[stage]),\n",
        "                    ('classifier', LogisticRegression(max_iter=1))\n",
        "                ])\n",
        "            else:\n",
        "                # Later stages use more complex models for better accuracy\n",
        "                return Pipeline([\n",
        "                    ('vectorizer', self.vectorizers[stage]),\n",
        "                    ('classifier', RandomForestClassifier(n_estimators=100))\n",
        "                ])\n",
        "\n",
        "    def train_models(self, train_urls, train_labels):\n",
        "        \"\"\"\n",
        "        Train the cascade of ML models.\n",
        "\n",
        "        Args:\n",
        "            train_urls: List of URLs for training\n",
        "            train_labels: Binary labels (0 for safe, 1 for unsafe)\n",
        "        \"\"\"\n",
        "        print(\"Training cascade of ML models...\")\n",
        "\n",
        "        # Convert to numpy arrays if they're not already\n",
        "        train_urls = np.array(train_urls)\n",
        "        train_labels = np.array(train_labels)\n",
        "\n",
        "        # Keep track of remaining data for each stage\n",
        "        remaining_urls = train_urls\n",
        "        remaining_labels = train_labels\n",
        "        \n",
        "        # Keep track of URLs that pass the final stage but are actually malicious\n",
        "        final_stage_false_negatives = []\n",
        "\n",
        "        # Train each stage\n",
        "        for stage in range(self.num_stages):\n",
        "            print(f\"Training stage {stage+1}/{self.num_stages}...\")\n",
        "\n",
        "            # Create and train the model for this stage\n",
        "            self.models[stage] = self._create_model(stage)\n",
        "\n",
        "            # First, fit the vectorizer to transform text data\n",
        "            print(f\"Fitting vectorizer for stage {stage+1}...\")\n",
        "            X = self.vectorizers[stage].fit_transform(remaining_urls)\n",
        "\n",
        "            if self.device.type == 'cpu':\n",
        "                # CPU training with scikit-learn\n",
        "                print(f\"Fitting model for stage {stage+1} using CPU...\")\n",
        "                # For scikit-learn, we need to fit the classifier part of the pipeline\n",
        "                self.models[stage].named_steps['classifier'].fit(X, remaining_labels)\n",
        "\n",
        "                # Get predicted probabilities\n",
        "                print(f\"Getting predicted probabilities for stage {stage+1}...\")\n",
        "                probs = np.zeros(len(remaining_urls))\n",
        "                progress_bar = tqdm(total=len(remaining_urls), desc=f\"Predicting probabilities (stage {stage+1})\", unit=\"url\")\n",
        "\n",
        "                # Process in batches to show progress\n",
        "                batch_size = 1000\n",
        "                for i in range(0, len(remaining_urls), batch_size):\n",
        "                    batch_end = min(i + batch_size, len(remaining_urls))\n",
        "                    batch_vectors = X[i:batch_end]\n",
        "                    batch_probs = self.models[stage].named_steps['classifier'].predict_proba(batch_vectors)[:, 1]\n",
        "                    probs[i:batch_end] = batch_probs\n",
        "                    progress_bar.update((batch_vectors.shape[0]))\n",
        "\n",
        "                progress_bar.close()\n",
        "            else:\n",
        "                # GPU training with PyTorch\n",
        "                print(f\"Fitting model for stage {stage+1} using GPU...\")\n",
        "\n",
        "                # Convert to PyTorch tensors\n",
        "                X_tensor = torch.FloatTensor(X.toarray()).to(self.device)\n",
        "                y_tensor = torch.FloatTensor(remaining_labels.reshape(-1, 1)).to(self.device)\n",
        "\n",
        "                # Create PyTorch model based on stage\n",
        "                input_dim = X.shape[1]\n",
        "                if stage == 0:\n",
        "                    # First stage uses a simpler model for faster processing\n",
        "                    self.pytorch_models[stage] = PyTorchLogisticRegression(input_dim).to(self.device)\n",
        "                else:\n",
        "                    # Later stages use more complex models for better accuracy\n",
        "                    self.pytorch_models[stage] = PyTorchRandomForest(input_dim).to(self.device)\n",
        "\n",
        "                # Define loss function and optimizer\n",
        "                criterion = nn.BCELoss()\n",
        "                optimizer = optim.Adam(self.pytorch_models[stage].parameters(), lr=0.01)\n",
        "\n",
        "                # Create DataLoader for batch processing\n",
        "                dataset = TensorDataset(X_tensor, y_tensor)\n",
        "                dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "                # Train the model\n",
        "                print(f\"Training PyTorch model for stage {stage+1}...\")\n",
        "                num_epochs = 1\n",
        "                for epoch in range(num_epochs):\n",
        "                    running_loss = 0.0\n",
        "                    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "\n",
        "                    for inputs, labels in progress_bar:\n",
        "                        # Zero the parameter gradients\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        # Forward pass\n",
        "                        outputs = self.pytorch_models[stage](inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # Backward pass and optimize\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        # Update statistics\n",
        "                        running_loss += loss.item() * inputs.size(0)\n",
        "                        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "                    epoch_loss = running_loss / len(dataset)\n",
        "                    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "                # Set model to evaluation mode\n",
        "                self.pytorch_models[stage].eval()\n",
        "\n",
        "                # Get predicted probabilities\n",
        "                print(f\"Getting predicted probabilities for stage {stage+1}...\")\n",
        "                probs = np.zeros(len(remaining_urls))\n",
        "                progress_bar = tqdm(total=len(remaining_urls), desc=f\"Predicting probabilities (stage {stage+1})\", unit=\"url\")\n",
        "\n",
        "                # Process in batches to show progress\n",
        "                batch_size = 1000\n",
        "                with torch.no_grad():\n",
        "                    for i in range(0, len(remaining_urls), batch_size):\n",
        "                        batch_end = min(i + batch_size, len(remaining_urls))\n",
        "                        batch_vectors = X[i:batch_end].toarray()\n",
        "                        batch_tensor = torch.FloatTensor(batch_vectors).to(self.device)\n",
        "                        batch_probs = self.pytorch_models[stage](batch_tensor).cpu().numpy().flatten()\n",
        "                        probs[i:batch_end] = batch_probs\n",
        "                        progress_bar.update((batch_vectors.shape[0]))\n",
        "\n",
        "                progress_bar.close()\n",
        "\n",
        "            # Find optimal threshold using ROC curve\n",
        "            fpr, tpr, thresholds = roc_curve(remaining_labels, probs)\n",
        "\n",
        "            # Find threshold that gives desired false positive rate\n",
        "            target_fpr = self.error_rate / self.num_stages  # Distribute error rate across stages\n",
        "            idx = np.argmin(np.abs(fpr - target_fpr))\n",
        "            self.thresholds[stage] = thresholds[idx]\n",
        "\n",
        "            print(f\"Optimal threshold for stage {stage+1}: {self.thresholds[stage]:.4f}\")\n",
        "\n",
        "            # Identify items that pass this stage (high probability)\n",
        "            passed_mask = probs >= self.thresholds[stage]\n",
        "            passed_urls = remaining_urls[passed_mask]\n",
        "            passed_labels = remaining_labels[passed_mask]\n",
        "\n",
        "            # Handle false negatives (items that are actually malicious but were classified as benign)\n",
        "            # For all stages, we need to track these\n",
        "            false_negative_mask = (~passed_mask) & (remaining_labels == 1)\n",
        "            false_negative_urls = remaining_urls[false_negative_mask]\n",
        "            \n",
        "            # Create Bloom filter for this stage\n",
        "            if len(false_negative_urls) > 0:\n",
        "                stage_error_rate = self.error_rate / self.num_stages\n",
        "                # Use a reasonable initial capacity but allow growth\n",
        "                initial_capacity = max(len(false_negative_urls), 100)\n",
        "                bloom = ScalableBloomFilter(initial_capacity=initial_capacity, error_rate=stage_error_rate)\n",
        "\n",
        "                # Add false negative items to Bloom filter\n",
        "                print(f\"Adding {len(false_negative_urls)} false negatives to Bloom filter for stage {stage+1}...\")\n",
        "                progress_bar = tqdm(total=len(false_negative_urls), desc=f\"Adding to Bloom filter (stage {stage+1})\", unit=\"url\")\n",
        "\n",
        "                for url in false_negative_urls:\n",
        "                    bloom.add(url)\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "                progress_bar.close()\n",
        "\n",
        "                self.bloom_filters.append(bloom)\n",
        "                \n",
        "                # If this is the final stage, these are our final false negatives\n",
        "                if stage == self.num_stages - 1:\n",
        "                    final_stage_false_negatives = false_negative_urls\n",
        "            else:\n",
        "                # No false negatives to add to Bloom filter\n",
        "                self.bloom_filters.append(None)\n",
        "\n",
        "            # Special handling for the final stage\n",
        "            if stage == self.num_stages - 1:\n",
        "                # Track stats for final stage false negatives\n",
        "                print(f\"Final stage false negatives detected: {len(final_stage_false_negatives)}\")\n",
        "                self.false_negatives_detected = len(final_stage_false_negatives)\n",
        "                \n",
        "                # Final stage also needs to catch predicted safe but actually malicious URLs\n",
        "                final_false_negative_mask = passed_mask & (remaining_labels == 1)\n",
        "                final_false_negative_urls = remaining_urls[final_false_negative_mask]\n",
        "                \n",
        "                if len(final_false_negative_urls) > 0:\n",
        "                    print(f\"Adding {len(final_false_negative_urls)} additional malicious URLs to final safety net Bloom filter...\")\n",
        "                    \n",
        "                    # Create additional safety net Bloom filter if needed\n",
        "                    if self.bloom_filters[-1] is None:\n",
        "                        # Create new Bloom filter for safety net\n",
        "                        safety_net_error_rate = self.error_rate / 2  # Stricter error rate for safety net\n",
        "                        safety_net_capacity = max(len(final_false_negative_urls), 100)\n",
        "                        safety_net_bloom = ScalableBloomFilter(initial_capacity=safety_net_capacity, error_rate=safety_net_error_rate)\n",
        "                        self.bloom_filters[-1] = safety_net_bloom\n",
        "                    \n",
        "                    # Add URLs to the safety net Bloom filter\n",
        "                    progress_bar = tqdm(total=len(final_false_negative_urls), desc=\"Adding to safety net Bloom filter\", unit=\"url\")\n",
        "                    for url in final_false_negative_urls:\n",
        "                        self.bloom_filters[-1].add(url)\n",
        "                        progress_bar.update(1)\n",
        "                    progress_bar.close()\n",
        "\n",
        "            # Update remaining data for next stage\n",
        "            if stage < self.num_stages - 1:\n",
        "                remaining_urls = passed_urls\n",
        "                remaining_labels = passed_labels\n",
        "                print(f\"Items passing to stage {stage+2}: {len(remaining_urls)} ({len(remaining_urls)/len(train_urls)*100:.2f}% of original data)\")\n",
        "\n",
        "                if len(remaining_urls) == 0:\n",
        "                    print(f\"No items left for stage {stage+2}. Stopping cascade training.\")\n",
        "                    break\n",
        "\n",
        "        print(\"Cascade training complete.\")\n",
        "\n",
        "    def _get_probability(self, url, stage):\n",
        "        \"\"\"\n",
        "        Get the probability from the model for a single URL at a specific stage.\n",
        "\n",
        "        Args:\n",
        "            url: URL to get probability for\n",
        "            stage: Stage index\n",
        "\n",
        "        Returns:\n",
        "            Probability value\n",
        "        \"\"\"\n",
        "        if self.device.type == 'cpu' or self.pytorch_models[stage] is None:\n",
        "            # Use scikit-learn model\n",
        "            url_vector = self.vectorizers[stage].transform([url])\n",
        "            return self.models[stage].named_steps['classifier'].predict_proba(url_vector)[:, 1][0]\n",
        "        else:\n",
        "            # Use PyTorch model\n",
        "            url_vector = self.vectorizers[stage].transform([url]).toarray()\n",
        "            url_tensor = torch.FloatTensor(url_vector).to(self.device)\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                prob = self.pytorch_models[stage](url_tensor).cpu().numpy()[0, 0]\n",
        "\n",
        "            return prob\n",
        "\n",
        "    def add(self, url):\n",
        "        \"\"\"\n",
        "        Add a URL to the filter.\n",
        "\n",
        "        Args:\n",
        "            url: URL to add\n",
        "        \"\"\"\n",
        "        if (self.device.type == 'cpu' and not self.models[0]) or \\\n",
        "           (self.device.type == 'cuda' and not self.pytorch_models[0]):\n",
        "            raise ValueError(\"Models must be trained before adding elements\")\n",
        "\n",
        "        # Process through the cascade\n",
        "        for stage in range(self.num_stages):\n",
        "            # Get probability from model\n",
        "            prob = self._get_probability(url, stage)\n",
        "\n",
        "            # Update statistics\n",
        "            self.stage_stats[stage]['items_processed'] += 1\n",
        "\n",
        "            if prob < self.thresholds[stage]:\n",
        "                # Item rejected by this stage's model\n",
        "                self.stage_stats[stage]['items_rejected'] += 1\n",
        "\n",
        "                # Add to Bloom filter if this is a known malicious URL\n",
        "                if self.bloom_filters[stage]:\n",
        "                    self.bloom_filters[stage].add(url)\n",
        "\n",
        "                break\n",
        "            else:\n",
        "                # Item passed this stage\n",
        "                self.stage_stats[stage]['items_passed'] += 1\n",
        "\n",
        "                # If this is the last stage, we're done\n",
        "                if stage == self.num_stages - 1:\n",
        "                    # For the final stage, we add known malicious URLs to the safety net Bloom filter\n",
        "                    if self.bloom_filters[stage]:\n",
        "                        self.bloom_filters[stage].add(url)\n",
        "                    break\n",
        "\n",
        "        # Update total items added\n",
        "        self.items_added += 1\n",
        "\n",
        "    def query(self, url):\n",
        "        \"\"\"\n",
        "        Query if a URL is potentially malicious.\n",
        "\n",
        "        Args:\n",
        "            url: URL to query\n",
        "\n",
        "        Returns:\n",
        "            True if the URL might be malicious, False if definitely benign\n",
        "        \"\"\"\n",
        "        if (self.device.type == 'cpu' and not self.models[0]) or \\\n",
        "           (self.device.type == 'cuda' and not self.pytorch_models[0]):\n",
        "            raise ValueError(\"Models must be trained before querying\")\n",
        "\n",
        "        # Process through the cascade\n",
        "        for stage in range(self.num_stages):\n",
        "            # Get probability from model\n",
        "            prob = self._get_probability(url, stage)\n",
        "\n",
        "            if prob < self.thresholds[stage]:\n",
        "                # Item rejected by this stage's model\n",
        "                \n",
        "                # Check Bloom filter for this stage to see if it's a false negative\n",
        "                if self.bloom_filters[stage] and url in self.bloom_filters[stage]:\n",
        "                    # URL is in the Bloom filter, so it's potentially malicious\n",
        "                    return True\n",
        "                else:\n",
        "                    # URL is not in the Bloom filter, so it's definitely benign\n",
        "                    return False\n",
        "            else:\n",
        "                # Item passed this stage, continue to next stage\n",
        "                if stage == self.num_stages - 1:\n",
        "                    # Final stage classifier says it's malicious\n",
        "                    return True\n",
        "                    \n",
        "                    # Note: We don't need to check the final Bloom filter here\n",
        "                    # because if the final classifier says it's malicious, we\n",
        "                    # trust that decision. The final Bloom filter is only used\n",
        "                    # to catch false negatives that the classifier thought were benign.\n",
        "\n",
        "        # Should never reach here\n",
        "        return False\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Return statistics about the filter.\"\"\"\n",
        "        # Calculate total bits used by all Bloom filters\n",
        "        total_bits = 0\n",
        "        bloom_filter_sizes = []\n",
        "\n",
        "        for bf in self.bloom_filters:\n",
        "            if bf:\n",
        "                # ScalableBloomFilter doesn't have a bitarray attribute\n",
        "                # Instead, it has a series of BloomFilter objects in its 'filters' attribute\n",
        "                if hasattr(bf, 'filters'):\n",
        "                    # For ScalableBloomFilter\n",
        "                    bf_bits = sum(len(f.bitarray) for f in bf.filters)\n",
        "                else:\n",
        "                    # For regular BloomFilter (fallback)\n",
        "                    bf_bits = len(bf.bitarray)\n",
        "\n",
        "                total_bits += bf_bits\n",
        "                bloom_filter_sizes.append(bf_bits / 8 / 1024 / 1024)  # Convert to MB\n",
        "            else:\n",
        "                bloom_filter_sizes.append(0)\n",
        "\n",
        "        total_size_mb = total_bits / 8 / 1024 / 1024  # Convert bits to MB\n",
        "\n",
        "        # Calculate bits per item\n",
        "        bits_per_item = total_bits / self.items_added if self.items_added > 0 else 0\n",
        "\n",
        "        stats = {\n",
        "            \"total_items\": self.items_added,\n",
        "            \"num_stages\": self.num_stages,\n",
        "            \"thresholds\": self.thresholds,\n",
        "            \"stage_stats\": self.stage_stats,\n",
        "            \"error_rate\": self.error_rate,\n",
        "            \"total_bits\": total_bits,\n",
        "            \"total_size_mb\": total_size_mb,\n",
        "            \"bits_per_item\": bits_per_item,\n",
        "            \"bloom_filter_sizes_mb\": bloom_filter_sizes,\n",
        "            \"false_negatives_detected\": self.false_negatives_detected\n",
        "        }\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_cascaded_learned_bloom_filter(train_urls, train_labels, test_urls, test_labels, capacity, error_rate=0.01, num_stages=2):\n",
        "    \"\"\"\n",
        "    Evaluate a Cascaded Learned Bloom Filter.\n",
        "\n",
        "    Args:\n",
        "        train_urls: URLs for training\n",
        "        train_labels: Labels for training (0 for safe, 1 for unsafe)\n",
        "        test_urls: URLs for testing\n",
        "        test_labels: Labels for testing (not used in current implementation but included for future label-specific analysis)\n",
        "        capacity: Expected number of elements\n",
        "        error_rate: Desired false positive rate\n",
        "        num_stages: Number of cascade stages\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nEvaluating Cascaded Learned Bloom Filter with {num_stages} stages and error_rate={error_rate}\")\n",
        "\n",
        "    # Initialize memory and time tracking\n",
        "    start_time = time.time()\n",
        "    memory_before = get_memory_usage()\n",
        "    print(f\"Initial memory usage: {memory_before:.2f} MB\")\n",
        "\n",
        "    # Create and train CLBF\n",
        "    clbf = CascadedLearnedBloomFilter(capacity, error_rate, num_stages)\n",
        "    clbf.train_models(train_urls, train_labels)\n",
        "\n",
        "    # Track memory after model creation\n",
        "    memory_after_model = get_memory_usage()\n",
        "    print(f\"Memory after model creation: {memory_after_model:.2f} MB\")\n",
        "    model_creation_time = time.time() - start_time\n",
        "\n",
        "    unsafe_train_urls = [url for url, label in zip(train_urls, train_labels) if label == 1]\n",
        "    # Add all training URLs to the filter\n",
        "\n",
        "    print(f\"Adding {len(unsafe_train_urls)} URLs to the filter...\")\n",
        "    add_start_time = time.time()\n",
        "    progress_bar = tqdm(total=len(unsafe_train_urls), desc=\"Adding URLs\", unit=\"url\")\n",
        "    \n",
        "    # Process in batches for better progress tracking\n",
        "    batch_size = 1000\n",
        "    for i in range(0, len(unsafe_train_urls), batch_size):\n",
        "        batch_end = min(i + batch_size, len(unsafe_train_urls))\n",
        "        for url in unsafe_train_urls[i:batch_end]:\n",
        "            clbf.add(url)\n",
        "        progress_bar.update(batch_end - i)\n",
        "\n",
        "    progress_bar.close()\n",
        "    \n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    add_time = time.time() - add_start_time\n",
        "    memory_after_add = get_memory_usage()\n",
        "    print(f\"Memory after adding URLs: {memory_after_add:.2f} MB\")\n",
        "\n",
        "        # Test on the test_urls\n",
        "    print(f\"Testing {len(test_urls)} URLs against the filter...\")\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    progress_bar = tqdm(total=len(test_urls), desc=\"Testing URLs\", unit=\"url\")\n",
        "    for url, label in zip(test_urls, test_labels):\n",
        "        result = clbf.query(url)  # Predicts: unsafe (True) or safe (False)\n",
        "\n",
        "        if result:  # Bloom filter says: possibly unsafe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                true_positives += 1\n",
        "            else:           # Actually safe\n",
        "                false_positives += 1\n",
        "        else:  # Bloom filter says: definitely safe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                false_negatives += 1\n",
        "            else:           # Actually safe\n",
        "                true_negatives += 1\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Tested {len(test_urls)} URLs in {test_time:.2f} seconds\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    false_positive_rate = false_positives / len(test_urls) if len(test_urls) > 0 else 0\n",
        "    false_negative_rate = false_negatives / len(train_urls) if len(train_urls) > 0 else 0\n",
        "    # bits_per_item = filter_bits / len(train_urls) if len(train_urls) > 0 else 0\n",
        "\n",
        "    # Get filter stats\n",
        "    filter_stats = clbf.get_stats()\n",
        "\n",
        "    # Prepare results\n",
        "    results = {\n",
        "        \"filter_type\": f\"CLBF (stages={num_stages}, error_rate={error_rate})\",\n",
        "        \"num_stages\": num_stages,\n",
        "        \"error_rate\": error_rate,\n",
        "        \"capacity\": capacity,\n",
        "        \"memory_before\": memory_before,\n",
        "        \"memory_after_model\": memory_after_model,\n",
        "        \"memory_after_add\": memory_after_add,\n",
        "        \"memory_for_model\": memory_after_model - memory_before,\n",
        "        \"memory_for_filters\": filter_stats[\"total_size_mb\"],\n",
        "        \"total_memory\": (memory_after_model - memory_before) + filter_stats[\"total_size_mb\"],\n",
        "        \"model_creation_time\": model_creation_time,\n",
        "        \"add_time\": add_time,\n",
        "        \"true_positives\": true_positives,\n",
        "        \"false_negatives\": false_negatives,\n",
        "        \"true_negatives\": true_negatives,\n",
        "        \"false_positives\": false_positives,\n",
        "        \"false_positive_rate\": false_positive_rate,\n",
        "        \"false_negative_rate\": false_negative_rate,\n",
        "        \"stage_stats\": filter_stats[\"stage_stats\"],\n",
        "        \"thresholds\": filter_stats[\"thresholds\"],\n",
        "        \"bits_per_item\": filter_stats[\"bits_per_item\"],\n",
        "        \"test_time\": test_time,\n",
        "        \"total_bits\": filter_stats[\"total_bits\"],\n",
        "        \"accuracy\": (true_positives + true_negatives) / len(test_urls) if len(test_urls) > 0 else 0,\n",
        "        \"precision\": true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
        "        \"recall\": true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
        "        \"f1_score\": (2 * true_positives) / (2 * true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0,\n",
        "    \n",
        "    }\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"  False Positive Rate: {false_positive_rate:.4f} (expected: {error_rate})\")\n",
        "    print(f\"  False Negative Rate: {false_negative_rate:.4f}\")\n",
        "    print(f\"  Memory for model: {results['memory_for_model']:.2f} MB\")\n",
        "    print(f\"  Memory for Bloom filters: {results['memory_for_filters']:.2f} MB\")\n",
        "    print(f\"  Total memory (model + filters): {results['total_memory']:.2f} MB\")\n",
        "    print(f\"  Bits per item: {results['bits_per_item']:.2f}\")\n",
        "    print(f\"  Model creation time: {model_creation_time:.2f} seconds\")\n",
        "    print(f\"  Add time: {add_time:.2f} seconds\")\n",
        "    print(f\"  Test time: {results['test_time']} seconds\")\n",
        "    print(f\"  True positives: {results['true_positives']}\")\n",
        "    print(f\"  False positives: {results['false_positives']}\")\n",
        "    print(f\"  True negatives: {results['true_negatives']}\")\n",
        "    print(f\"  False negatives: {results['false_negatives']}\")\n",
        "    print(f\"  False positive rate: {results['false_positive_rate']:.4f} (expected: {results['error_rate']})\")\n",
        "    print(f\"  False negative rate: {results['false_negative_rate']:.4f}\")\n",
        "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
        "    print(f\"  Stage statistics:\")\n",
        "    for stage in range(num_stages):\n",
        "        stats = filter_stats[\"stage_stats\"][stage]\n",
        "        print(f\"    Stage {stage+1}: Processed {stats['items_processed']}, \"\n",
        "              f\"Rejected {stats['items_rejected']} ({stats['items_rejected']/stats['items_processed']*100 if stats['items_processed'] > 0 else 0:.2f}%), \"\n",
        "              f\"Passed {stats['items_passed']} ({stats['items_passed']/stats['items_processed']*100 if stats['items_processed'] > 0 else 0:.2f}%)\")\n",
        "    print(f\"  Bloom filter sizes: {[f'{size:.2f} MB' for size in filter_stats['bloom_filter_sizes_mb']]}\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DMLBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class EntropyCalculator:\n",
        "    \"\"\"Calculates and tracks entropy for data partitions.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.partition_histograms = defaultdict(lambda: defaultdict(int))\n",
        "        self.partition_counts = defaultdict(int)\n",
        "        self.partition_entropies = {}\n",
        "    \n",
        "    def update(self, partition_id, element_hash):\n",
        "        \"\"\"Update histograms with new element.\"\"\"\n",
        "        # Use a binned hash value to track distribution\n",
        "        binned_hash = element_hash % 1000  # Create 1000 bins for histogram\n",
        "        self.partition_histograms[partition_id][binned_hash] += 1\n",
        "        self.partition_counts[partition_id] += 1\n",
        "        # Recalculate entropy for this partition\n",
        "        self._calculate_entropy(partition_id)\n",
        "    \n",
        "    def _calculate_entropy(self, partition_id):\n",
        "        \"\"\"Calculate Shannon entropy for a partition.\"\"\"\n",
        "        histogram = self.partition_histograms[partition_id]\n",
        "        total_count = self.partition_counts[partition_id]\n",
        "        \n",
        "        if total_count == 0:\n",
        "            self.partition_entropies[partition_id] = 0\n",
        "            return\n",
        "        \n",
        "        entropy = 0\n",
        "        for count in histogram.values():\n",
        "            probability = count / total_count\n",
        "            if probability > 0:\n",
        "                entropy -= probability * math.log2(probability)\n",
        "        \n",
        "        self.partition_entropies[partition_id] = entropy\n",
        "    \n",
        "    def get_entropy(self, partition_id):\n",
        "        \"\"\"Get entropy for a specific partition.\"\"\"\n",
        "        return self.partition_entropies.get(partition_id, 0)\n",
        "    \n",
        "    def get_all_entropies(self):\n",
        "        \"\"\"Get entropies for all partitions.\"\"\"\n",
        "        return self.partition_entropies\n",
        "\n",
        "class AdaptivePartitioningModel(nn.Module):\n",
        "    \"\"\"Neural network that learns to assign elements to partitions based on features.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, num_partitions):\n",
        "        super(AdaptivePartitioningModel, self).__init__()\n",
        "        self.num_partitions = num_partitions\n",
        "        \n",
        "        # Feature extraction layers\n",
        "        self.feature_layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        \n",
        "        # Partition assignment layer\n",
        "        self.partition_layer = nn.Linear(128, num_partitions)\n",
        "        \n",
        "        # Confidence layer (for entropy-aware thresholding)\n",
        "        self.confidence_layer = nn.Linear(128, 1)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.feature_layers(x)\n",
        "        partition_logits = self.partition_layer(features)\n",
        "        partition_probs = self.softmax(partition_logits)\n",
        "        confidence = self.sigmoid(self.confidence_layer(features))\n",
        "        \n",
        "        return partition_probs, confidence\n",
        "\n",
        "class CascadingBloomFilter:\n",
        "    \"\"\"Multi-level Bloom filter with cascading structure.\"\"\"\n",
        "    \n",
        "    def __init__(self, initial_capacity, error_rate, num_levels=3):\n",
        "        self.num_levels = num_levels\n",
        "        self.filters = []\n",
        "        \n",
        "        # Create a cascade of increasingly precise Bloom filters\n",
        "        for level in range(num_levels):\n",
        "            # Each level has lower error rate but smaller capacity\n",
        "            level_error = error_rate / (level + 1)\n",
        "            level_capacity = initial_capacity // (2 ** level)\n",
        "            \n",
        "            self.filters.append(\n",
        "                ScalableBloomFilter(\n",
        "                    initial_capacity=max(level_capacity, 100),\n",
        "                    error_rate=level_error\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        self.items_added = 0\n",
        "    \n",
        "    def add(self, element):\n",
        "        \"\"\"Add element to all levels of the cascade.\"\"\"\n",
        "        for filter_level in self.filters:\n",
        "            filter_level.add(element)\n",
        "        self.items_added += 1\n",
        "    \n",
        "    def check(self, element, max_level=None):\n",
        "        \"\"\"Check if element exists in the cascade, up to max_level.\"\"\"\n",
        "        if max_level is None:\n",
        "            max_level = self.num_levels\n",
        "        \n",
        "        # Check each level in sequence\n",
        "        for level in range(min(max_level, self.num_levels)):\n",
        "            if element not in self.filters[level]:\n",
        "                return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def get_size_bits(self):\n",
        "        \"\"\"Get total size in bits.\"\"\"\n",
        "        total_bits = 0\n",
        "        for filter_level in self.filters:\n",
        "            if hasattr(filter_level, 'filters'):\n",
        "                # For ScalableBloomFilter\n",
        "                level_bits = sum(len(f.bitarray) for f in filter_level.filters)\n",
        "            else:\n",
        "                # For regular BloomFilter\n",
        "                level_bits = len(filter_level.bitarray)\n",
        "            total_bits += level_bits\n",
        "        \n",
        "        return total_bits\n",
        "\n",
        "class DynamicMultiLevelLearnedBloomFilter:\n",
        "    \"\"\"\n",
        "    Dynamic Multi-Level Learned Bloom Filter with Entropy Checking (DML-LBF)\n",
        "    \n",
        "    Combines learned models, partitioning, cascading, adaptivity, and entropy checking\n",
        "    into a unified system for efficient membership testing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, expected_capacity, error_rate=0.01, initial_partitions=8):\n",
        "        \"\"\"\n",
        "        Initialize the DML-LBF.\n",
        "        \n",
        "        Args:\n",
        "            expected_capacity: Expected number of elements\n",
        "            error_rate: Target false positive rate\n",
        "            initial_partitions: Initial number of partitions\n",
        "        \"\"\"\n",
        "        self.expected_capacity = expected_capacity\n",
        "        self.error_rate = error_rate\n",
        "        self.num_partitions = initial_partitions\n",
        "        \n",
        "        # Check if CUDA is available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        # Text vectorizer for feature extraction\n",
        "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
        "        \n",
        "        # Partitioning model (will be initialized during training)\n",
        "        self.model = None\n",
        "        \n",
        "        # Entropy tracker\n",
        "        self.entropy_calculator = EntropyCalculator()\n",
        "        \n",
        "        # Partition Bloom filters\n",
        "        self.partition_filters = {}\n",
        "        \n",
        "        # Partition metadata\n",
        "        self.partition_stats = defaultdict(lambda: {\n",
        "            'items_added': 0,\n",
        "            'queries': 0,\n",
        "            'hits': 0,\n",
        "            'last_accessed': time.time()\n",
        "        })\n",
        "        \n",
        "        # System-wide statistics\n",
        "        self.total_items = 0\n",
        "        self.total_queries = 0\n",
        "        self.maintenance_counter = 0\n",
        "        \n",
        "        # Adaptive thresholds\n",
        "        self.entropy_threshold_high = 4.0  # High entropy threshold for merging\n",
        "        self.entropy_threshold_low = 2.0   # Low entropy threshold for splitting\n",
        "        self.query_threshold = 1000        # Minimum queries before adaptation\n",
        "        self.maintenance_interval = 10000  # Items before maintenance check\n",
        "        \n",
        "        # Partition mapping (for when partitions are merged/split)\n",
        "        self.partition_mapping = {}  # Maps old partition IDs to new ones\n",
        "    \n",
        "    def _initialize_partition_filter(self, partition_id, capacity=None):\n",
        "        \"\"\"Initialize a new cascading Bloom filter for a partition.\"\"\"\n",
        "        if capacity is None:\n",
        "            capacity = max(self.expected_capacity // self.num_partitions, 100)\n",
        "        \n",
        "        self.partition_filters[partition_id] = CascadingBloomFilter(\n",
        "            initial_capacity=capacity,\n",
        "            error_rate=self.error_rate,\n",
        "            num_levels=3\n",
        "        )\n",
        "    \n",
        "    def train(self, training_data, labels=None):\n",
        "        \"\"\"\n",
        "        Train the partitioning model and initialize the system.\n",
        "        \n",
        "        Args:\n",
        "            training_data: List of elements for training\n",
        "            labels: Optional binary labels (1 for membership, 0 for non-membership)\n",
        "        \"\"\"\n",
        "        print(\"Training DML-LBF...\")\n",
        "        \n",
        "        # Step 1: Vectorize the training data\n",
        "        print(\"Vectorizing training data...\")\n",
        "        X = self.vectorizer.fit_transform(training_data)\n",
        "        input_dim = X.shape[1]\n",
        "        \n",
        "        # Step 2: Initialize and train the partitioning model\n",
        "        print(\"Training partitioning model...\")\n",
        "        self.model = AdaptivePartitioningModel(input_dim, self.num_partitions).to(self.device)\n",
        "        \n",
        "        # Convert to PyTorch tensors\n",
        "        X_tensor = torch.FloatTensor(X.toarray()).to(self.device)\n",
        "        \n",
        "        # If labels are provided, use them for supervised training\n",
        "        if labels is not None:\n",
        "            y_tensor = torch.FloatTensor(labels.reshape(-1, 1)).to(self.device)\n",
        "            dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        else:\n",
        "            # For unsupervised training, use dummy labels\n",
        "            dummy_labels = torch.zeros(X_tensor.shape[0], 1).to(self.device)\n",
        "            dataset = TensorDataset(X_tensor, dummy_labels)\n",
        "        \n",
        "        # Train the model\n",
        "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        \n",
        "        num_epochs = 5\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            \n",
        "            for inputs, _ in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Forward pass\n",
        "                partition_probs, _ = self.model(inputs)\n",
        "                \n",
        "                # For unsupervised learning, we want to maximize entropy of partition assignments\n",
        "                # This encourages balanced partitioning\n",
        "                partition_entropy = -torch.sum(partition_probs * torch.log(partition_probs + 1e-10), dim=1).mean()\n",
        "                loss = -partition_entropy  # Negative because we want to maximize entropy\n",
        "                \n",
        "                # Add regularization to prevent empty partitions\n",
        "                partition_counts = torch.sum(partition_probs, dim=0)\n",
        "                balance_penalty = torch.var(partition_counts) * 0.1\n",
        "                loss += balance_penalty\n",
        "                \n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n",
        "        \n",
        "        # Step 3: Determine initial partition assignments\n",
        "        print(\"Determining initial partition assignments...\")\n",
        "        partition_assignments = self._get_partition_assignments(X_tensor)\n",
        "        \n",
        "        # Step 4: Initialize Bloom filters for each partition\n",
        "        print(\"Initializing Bloom filters for each partition...\")\n",
        "        partition_counts = defaultdict(int)\n",
        "        for partition_id in partition_assignments:\n",
        "            partition_counts[partition_id] += 1\n",
        "        \n",
        "        for partition_id in range(self.num_partitions):\n",
        "            # Estimate capacity based on assignments\n",
        "            capacity = max(partition_counts.get(partition_id, 0) * 2, 100)\n",
        "            self._initialize_partition_filter(partition_id, capacity)\n",
        "        \n",
        "        # Step 5: Add training elements to appropriate partitions\n",
        "        print(\"Adding training elements to partitions...\")\n",
        "        progress_bar = tqdm(total=len(training_data), desc=\"Adding elements\")\n",
        "        \n",
        "        for i, element in enumerate(training_data):\n",
        "            partition_id = partition_assignments[i]\n",
        "            \n",
        "            # Add to appropriate Bloom filter\n",
        "            self.partition_filters[partition_id].add(element)\n",
        "            \n",
        "            # Update entropy calculation\n",
        "            element_hash = hash(element)\n",
        "            self.entropy_calculator.update(partition_id, element_hash)\n",
        "            \n",
        "            # Update statistics\n",
        "            self.partition_stats[partition_id]['items_added'] += 1\n",
        "            self.total_items += 1\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "        \n",
        "        progress_bar.close()\n",
        "        \n",
        "        # Step 6: Calculate initial entropy for each partition\n",
        "        print(\"Calculating initial entropy for each partition...\")\n",
        "        entropies = self.entropy_calculator.get_all_entropies()\n",
        "        for partition_id, entropy in entropies.items():\n",
        "            print(f\"Partition {partition_id}: Entropy = {entropy:.4f}, Items = {self.partition_stats[partition_id]['items_added']}\")\n",
        "        \n",
        "        print(\"DML-LBF training complete.\")\n",
        "    \n",
        "    def _get_partition_assignments(self, X_tensor):\n",
        "        \"\"\"Get partition assignments for a batch of elements.\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            partition_probs, _ = self.model(X_tensor)\n",
        "            partition_assignments = torch.argmax(partition_probs, dim=1).cpu().numpy()\n",
        "        return partition_assignments\n",
        "    \n",
        "    def _get_partition_probs(self, element):\n",
        "        \"\"\"Get partition probabilities for a single element.\"\"\"\n",
        "        # Vectorize the element\n",
        "        X = self.vectorizer.transform([element]).toarray()\n",
        "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
        "        \n",
        "        # Get predictions\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            partition_probs, confidence = self.model(X_tensor)\n",
        "            \n",
        "            # Convert to numpy for easier handling\n",
        "            partition_probs = partition_probs.cpu().numpy()[0]\n",
        "            confidence = confidence.cpu().numpy()[0][0]\n",
        "        \n",
        "        return partition_probs, confidence\n",
        "    \n",
        "    def _get_relevant_partitions(self, element, threshold=0.1):\n",
        "        \"\"\"\n",
        "        Get relevant partitions for an element based on model predictions.\n",
        "        \n",
        "        Returns partitions sorted by probability (highest first).\n",
        "        \"\"\"\n",
        "        partition_probs, confidence = self._get_partition_probs(element)\n",
        "        \n",
        "        # Adjust threshold based on confidence\n",
        "        adaptive_threshold = threshold * (1 - confidence * 0.5)\n",
        "        \n",
        "        # Get partitions with probability above threshold\n",
        "        relevant_partitions = []\n",
        "        for partition_id, prob in enumerate(partition_probs):\n",
        "            if prob >= adaptive_threshold:\n",
        "                # Include entropy in the sorting criteria\n",
        "                entropy = self.entropy_calculator.get_entropy(partition_id)\n",
        "                relevant_partitions.append((partition_id, prob, entropy))\n",
        "        \n",
        "        # Sort by probability (descending)\n",
        "        relevant_partitions.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # If no partitions meet the threshold, return the highest probability partition\n",
        "        if not relevant_partitions:\n",
        "            top_partition = np.argmax(partition_probs)\n",
        "            entropy = self.entropy_calculator.get_entropy(top_partition)\n",
        "            relevant_partitions = [(top_partition, partition_probs[top_partition], entropy)]\n",
        "        \n",
        "        return relevant_partitions\n",
        "    \n",
        "    def add(self, element):\n",
        "        \"\"\"Add an element to the filter.\"\"\"\n",
        "        # Get the most likely partition\n",
        "        relevant_partitions = self._get_relevant_partitions(element)\n",
        "        partition_id = relevant_partitions[0][0]\n",
        "        \n",
        "        # Ensure the partition exists\n",
        "        if partition_id not in self.partition_filters:\n",
        "            self._initialize_partition_filter(partition_id)\n",
        "        \n",
        "        # Add to the appropriate filter\n",
        "        self.partition_filters[partition_id].add(element)\n",
        "        \n",
        "        # Update entropy calculation\n",
        "        element_hash = hash(element)\n",
        "        self.entropy_calculator.update(partition_id, element_hash)\n",
        "        \n",
        "        # Update statistics\n",
        "        self.partition_stats[partition_id]['items_added'] += 1\n",
        "        self.total_items += 1\n",
        "        \n",
        "        # Check if maintenance is needed\n",
        "        self.maintenance_counter += 1\n",
        "        if self.maintenance_counter >= self.maintenance_interval:\n",
        "            self._perform_maintenance()\n",
        "            self.maintenance_counter = 0\n",
        "    \n",
        "    def query(self, element):\n",
        "        \"\"\"\n",
        "        Query if an element is in the filter.\n",
        "        \n",
        "        Returns True if the element might be in the filter, False if definitely not.\n",
        "        \"\"\"\n",
        "        # Get relevant partitions\n",
        "        relevant_partitions = self._get_relevant_partitions(element)\n",
        "        \n",
        "        # Update query statistics\n",
        "        self.total_queries += 1\n",
        "        \n",
        "        # Check each relevant partition, prioritizing by probability and entropy\n",
        "        # Sort by entropy (ascending) so we check low-entropy partitions first\n",
        "        # Low entropy partitions are more likely to give definitive answers\n",
        "        relevant_partitions.sort(key=lambda x: x[2])\n",
        "        \n",
        "        for partition_id, prob, entropy in relevant_partitions:\n",
        "            # Skip if partition doesn't exist\n",
        "            if partition_id not in self.partition_filters:\n",
        "                continue\n",
        "            \n",
        "            # Update partition statistics\n",
        "            self.partition_stats[partition_id]['queries'] += 1\n",
        "            self.partition_stats[partition_id]['last_accessed'] = time.time()\n",
        "            \n",
        "            # Determine how many levels to check based on entropy\n",
        "            # Lower entropy means we can check more levels for better accuracy\n",
        "            max_level = 3 if entropy < 3.0 else (2 if entropy < 4.0 else 1)\n",
        "            \n",
        "            # Check if element is in this partition's filter\n",
        "            if self.partition_filters[partition_id].check(element, max_level):\n",
        "                self.partition_stats[partition_id]['hits'] += 1\n",
        "                return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def _perform_maintenance(self):\n",
        "        \"\"\"Perform system maintenance: merge, split, or reconfigure partitions.\"\"\"\n",
        "        print(\"\\nPerforming system maintenance...\")\n",
        "        \n",
        "        # Get current entropies\n",
        "        entropies = self.entropy_calculator.get_all_entropies()\n",
        "        \n",
        "        # Identify candidates for splitting (low entropy, high query volume)\n",
        "        split_candidates = []\n",
        "        for partition_id, stats in self.partition_stats.items():\n",
        "            if partition_id not in entropies:\n",
        "                continue\n",
        "                \n",
        "            entropy = entropies[partition_id]\n",
        "            queries = stats['queries']\n",
        "            \n",
        "            if (entropy < self.entropy_threshold_low and \n",
        "                queries > self.query_threshold and \n",
        "                stats['items_added'] > 1000):\n",
        "                split_candidates.append((partition_id, entropy, queries))\n",
        "        \n",
        "        # Sort by entropy (ascending) and queries (descending)\n",
        "        split_candidates.sort(key=lambda x: (x[1], -x[2]))\n",
        "        \n",
        "        # Identify candidates for merging (high entropy, low query volume)\n",
        "        merge_candidates = []\n",
        "        for partition_id, stats in self.partition_stats.items():\n",
        "            if partition_id not in entropies:\n",
        "                continue\n",
        "                \n",
        "            entropy = entropies[partition_id]\n",
        "            queries = stats['queries']\n",
        "            \n",
        "            if (entropy > self.entropy_threshold_high or \n",
        "                (queries < self.query_threshold / 10 and stats['items_added'] > 0)):\n",
        "                merge_candidates.append((partition_id, entropy, queries))\n",
        "        \n",
        "        # Sort by entropy (descending) and queries (ascending)\n",
        "        merge_candidates.sort(key=lambda x: (-x[1], x[2]))\n",
        "        \n",
        "        # Perform splits (up to 2 per maintenance cycle)\n",
        "        splits_performed = 0\n",
        "        for partition_id, entropy, queries in split_candidates:\n",
        "            if splits_performed >= 2:\n",
        "                break\n",
        "                \n",
        "            print(f\"Splitting partition {partition_id} (entropy={entropy:.4f}, queries={queries})\")\n",
        "            self._split_partition(partition_id)\n",
        "            splits_performed += 1\n",
        "        \n",
        "        # Perform merges (up to 1 per maintenance cycle)\n",
        "        if len(merge_candidates) >= 2 and splits_performed < 2:\n",
        "            p1 = merge_candidates[0][0]\n",
        "            p2 = merge_candidates[1][0]\n",
        "            print(f\"Merging partitions {p1} and {p2}\")\n",
        "            self._merge_partitions(p1, p2)\n",
        "        \n",
        "        # Print maintenance summary\n",
        "        print(\"Maintenance complete.\")\n",
        "        print(f\"Total partitions: {len(self.partition_filters)}\")\n",
        "        print(f\"Total items: {self.total_items}\")\n",
        "        print(f\"Total queries: {self.total_queries}\")\n",
        "    \n",
        "    def _split_partition(self, partition_id):\n",
        "        \"\"\"Split a partition into two new partitions.\"\"\"\n",
        "        # This is a simplified implementation\n",
        "        # In a full implementation, we would:\n",
        "        # 1. Train a specialized model to split this partition\n",
        "        # 2. Reassign elements to the new partitions\n",
        "        # 3. Create new Bloom filters\n",
        "        \n",
        "        # For this implementation, we'll just create a new empty partition\n",
        "        new_partition_id = max(self.partition_filters.keys()) + 1\n",
        "        self._initialize_partition_filter(new_partition_id)\n",
        "        \n",
        "        # Update partition mapping\n",
        "        self.partition_mapping[partition_id] = [partition_id, new_partition_id]\n",
        "        \n",
        "        # Note: In a real implementation, we would need to redistribute elements\n",
        "        # between the original and new partition\n",
        "    \n",
        "    def _merge_partitions(self, partition_id1, partition_id2):\n",
        "        \"\"\"Merge two partitions into one.\"\"\"\n",
        "        # This is a simplified implementation\n",
        "        # In a full implementation, we would:\n",
        "        # 1. Create a new Bloom filter with combined capacity\n",
        "        # 2. Add all elements from both partitions\n",
        "        # 3. Update the model to reflect the merge\n",
        "        \n",
        "        # For this implementation, we'll just keep the first partition and remove the second\n",
        "        if partition_id2 in self.partition_filters:\n",
        "            del self.partition_filters[partition_id2]\n",
        "        \n",
        "        # Update partition mapping\n",
        "        self.partition_mapping[partition_id2] = [partition_id1]\n",
        "        \n",
        "        # Note: In a real implementation, we would need to combine the Bloom filters\n",
        "    \n",
        "    def retrain_model(self, training_data, labels=None):\n",
        "        \"\"\"Retrain the partitioning model with new data.\"\"\"\n",
        "        # This would be similar to the initial training but would preserve\n",
        "        # existing partitions and adapt the model to the current state\n",
        "        # Not fully implemented in this example\n",
        "        pass\n",
        "    \n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about the filter.\"\"\"\n",
        "        # Calculate total bits used by all Bloom filters\n",
        "        total_bits = 0\n",
        "        for partition_id, filter_obj in self.partition_filters.items():\n",
        "            total_bits += filter_obj.get_size_bits()\n",
        "        \n",
        "        # Calculate bits per item\n",
        "        bits_per_item = total_bits / self.total_items if self.total_items > 0 else 0\n",
        "        \n",
        "        # Get entropy statistics\n",
        "        entropies = self.entropy_calculator.get_all_entropies()\n",
        "        avg_entropy = sum(entropies.values()) / len(entropies) if entropies else 0\n",
        "        \n",
        "        # Compile partition statistics\n",
        "        partition_stats = {}\n",
        "        for partition_id, stats in self.partition_stats.items():\n",
        "            if partition_id in self.partition_filters:\n",
        "                partition_stats[partition_id] = {\n",
        "                    'items': stats['items_added'],\n",
        "                    'queries': stats['queries'],\n",
        "                    'hits': stats['hits'],\n",
        "                    'entropy': entropies.get(partition_id, 0),\n",
        "                    'bits': self.partition_filters[partition_id].get_size_bits()\n",
        "                }\n",
        "        \n",
        "        return {\n",
        "            'total_items': self.total_items,\n",
        "            'total_queries': self.total_queries,\n",
        "            'total_bits': total_bits,\n",
        "            'bits_per_item': bits_per_item,\n",
        "            'num_partitions': len(self.partition_filters),\n",
        "            'average_entropy': avg_entropy,\n",
        "            'partition_stats': partition_stats\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_dynamic_multilevel_learned_bloom_filter(train_urls, train_labels, test_urls, test_labels, capacity, error_rate=0.01, initial_partitions=8):\n",
        "    \"\"\"\n",
        "    Evaluate a Dynamic Multi-Level Learned Bloom Filter with Entropy Checking.\n",
        "    \n",
        "    Args:\n",
        "        train_urls: URLs for training\n",
        "        train_labels: Labels for training (0 for safe, 1 for unsafe)\n",
        "        test_urls: URLs for testing\n",
        "        test_labels: Labels for testing\n",
        "        capacity: Expected number of elements\n",
        "        error_rate: Desired false positive rate\n",
        "        initial_partitions: Initial number of partitions\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating DML-LBF with {initial_partitions} initial partitions and error_rate={error_rate}\")\n",
        "    \n",
        "    # Initialize memory and time tracking\n",
        "    start_time = time.time()\n",
        "    memory_before = get_memory_usage()\n",
        "    print(f\"Initial memory usage: {memory_before:.2f} MB\")\n",
        "    \n",
        "    # Create and train DML-LBF\n",
        "    dmlbf = DynamicMultiLevelLearnedBloomFilter(capacity, error_rate, initial_partitions)\n",
        "    dmlbf.train(train_urls, train_labels)\n",
        "    \n",
        "    # Track memory after model creation\n",
        "    memory_after_model = get_memory_usage()\n",
        "    print(f\"Memory after model creation: {memory_after_model:.2f} MB\")\n",
        "    model_creation_time = time.time() - start_time\n",
        "    unsafe_train_urls = [url for url, label in zip(train_urls, train_labels) if label == 1]\n",
        "    # Add all training URLs to the filter\n",
        "    print(f\"Adding {len(unsafe_train_urls)} URLs to the filter...\")\n",
        "    add_start_time = time.time()\n",
        "    progress_bar = tqdm(total=len(unsafe_train_urls), desc=\"Adding URLs\", unit=\"url\")\n",
        "    \n",
        "    # Process in batches for better progress tracking\n",
        "    batch_size = 1000\n",
        "    for i in range(0, len(unsafe_train_urls), batch_size):\n",
        "        batch_end = min(i + batch_size, len(unsafe_train_urls))\n",
        "        for url in unsafe_train_urls[i:batch_end]:\n",
        "            dmlbf.add(url)\n",
        "        progress_bar.update(batch_end - i)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    add_time = time.time() - add_start_time\n",
        "    memory_after_add = get_memory_usage()\n",
        "    print(f\"Memory after adding URLs: {memory_after_add:.2f} MB\")\n",
        "         # Test on the test_urls\n",
        "    print(f\"Testing {len(test_urls)} URLs against the filter...\")\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    true_negatives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    progress_bar = tqdm(total=len(test_urls), desc=\"Testing URLs\", unit=\"url\")\n",
        "    for url, label in zip(test_urls, test_labels):\n",
        "        result = dmlbf.query(url)  # Predicts: unsafe (True) or safe (False)\n",
        "\n",
        "        if result:  # Bloom filter says: possibly unsafe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                true_positives += 1\n",
        "            else:           # Actually safe\n",
        "                false_positives += 1\n",
        "        else:  # Bloom filter says: definitely safe\n",
        "            if label == 1:  # Actually unsafe\n",
        "                false_negatives += 1\n",
        "            else:           # Actually safe\n",
        "                true_negatives += 1\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Tested {len(test_urls)} URLs in {test_time:.2f} seconds\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    false_positive_rate = false_positives / len(test_urls) if len(test_urls) > 0 else 0\n",
        "    false_negative_rate = false_negatives / len(train_urls) if len(train_urls) > 0 else 0\n",
        "    # bits_per_item = filter_bits / len(train_urls) if len(train_urls) > 0 else 0\n",
        "\n",
        "    \n",
        "\n",
        "    # Get filter stats\n",
        "    filter_stats = dmlbf.get_stats()\n",
        "    \n",
        "    # Evaluate adaptation capabilities\n",
        "    print(\"Evaluating adaptation capabilities...\")\n",
        "    \n",
        "    # Simulate data distribution change by adding more items from test set\n",
        "    adaptation_sample = test_urls[:min(5000, len(test_urls))]\n",
        "    adaptation_start_time = time.time()\n",
        "    \n",
        "    progress_bar = tqdm(total=len(adaptation_sample), desc=\"Testing adaptation\", unit=\"url\")\n",
        "    for url in adaptation_sample:\n",
        "        dmlbf.add(url)\n",
        "        progress_bar.update(1)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    adaptation_time = time.time() - adaptation_start_time\n",
        "    \n",
        "    # Get updated stats after adaptation\n",
        "    updated_stats = dmlbf.get_stats()\n",
        "    \n",
        "    # Evaluate entropy-based optimization\n",
        "    print(\"Evaluating entropy-based optimization...\")\n",
        "    \n",
        "    # Test query times for different entropy partitions\n",
        "    entropy_values = list(filter_stats[\"partition_stats\"].values())\n",
        "    entropy_values.sort(key=lambda x: x[\"entropy\"])\n",
        "    \n",
        "    low_entropy_partitions = [p for p, stats in filter_stats[\"partition_stats\"].items() \n",
        "                             if stats[\"entropy\"] < filter_stats[\"average_entropy\"] * 0.7]\n",
        "    high_entropy_partitions = [p for p, stats in filter_stats[\"partition_stats\"].items() \n",
        "                              if stats[\"entropy\"] > filter_stats[\"average_entropy\"] * 1.3]\n",
        "    \n",
        "    # Test query times for low entropy partitions\n",
        "    low_entropy_query_times = []\n",
        "    if low_entropy_partitions:\n",
        "        sample_size = min(1000, len(train_urls))\n",
        "        sample_indices = np.random.choice(len(train_urls), sample_size, replace=False)\n",
        "        low_entropy_start_time = time.time()\n",
        "        \n",
        "        for i in sample_indices:\n",
        "            dmlbf.query(train_urls[i])\n",
        "            \n",
        "        low_entropy_query_times = (time.time() - low_entropy_start_time) / sample_size\n",
        "    \n",
        "    # Test query times for high entropy partitions\n",
        "    high_entropy_query_times = []\n",
        "    if high_entropy_partitions:\n",
        "        sample_size = min(1000, len(train_urls))\n",
        "        sample_indices = np.random.choice(len(train_urls), sample_size, replace=False)\n",
        "        high_entropy_start_time = time.time()\n",
        "        \n",
        "        for i in sample_indices:\n",
        "            dmlbf.query(train_urls[i])\n",
        "            \n",
        "        high_entropy_query_times = (time.time() - high_entropy_start_time) / sample_size\n",
        "    \n",
        "    # Prepare results\n",
        "    results = {\n",
        "        \"filter_type\": f\"DML-LBF (partitions={initial_partitions}, error_rate={error_rate})\",\n",
        "        \"initial_partitions\": initial_partitions,\n",
        "        \"error_rate\": error_rate,\n",
        "        \"capacity\": capacity,\n",
        "        \"memory_before\": memory_before,\n",
        "        \"memory_after_model\": memory_after_model,\n",
        "        \"memory_after_add\": memory_after_add,\n",
        "        \"memory_for_model\": memory_after_model - memory_before,\n",
        "        \"memory_for_filters\": filter_stats[\"total_bits\"] / 8 / 1024 / 1024,  # Convert bits to MB\n",
        "        \"total_memory\": (memory_after_model - memory_before) + (filter_stats[\"total_bits\"] / 8 / 1024 / 1024),\n",
        "        \"model_creation_time\": model_creation_time,\n",
        "        \"add_time\": add_time,\n",
        "        \"true_positives\": true_positives,\n",
        "        \"false_negatives\": false_negatives,\n",
        "        \"true_negatives\": true_negatives,\n",
        "        \"false_positives\": false_positives,\n",
        "        \"false_positive_rate\": false_positive_rate,\n",
        "        \"false_negative_rate\": false_negative_rate,\n",
        "        \"bits_per_item\": filter_stats[\"bits_per_item\"],\n",
        "        \"total_bits\": filter_stats[\"total_bits\"],\n",
        "        \"num_partitions_before\": len(filter_stats[\"partition_stats\"]),\n",
        "        \"num_partitions_after\": len(updated_stats[\"partition_stats\"]),\n",
        "        \"adaptation_time\": adaptation_time,\n",
        "        \"average_entropy_before\": filter_stats[\"average_entropy\"],\n",
        "        \"average_entropy_after\": updated_stats[\"average_entropy\"],\n",
        "        \"low_entropy_query_time\": low_entropy_query_times,\n",
        "        \"high_entropy_query_time\": high_entropy_query_times,\n",
        "        'test_time':test_time,\n",
        "        \"accuracy\": (true_positives + true_negatives) / len(test_urls) if len(test_urls) > 0 else 0,\n",
        "        \"precision\": true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
        "        \"recall\": true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
        "        \"f1_score\": (2 * true_positives) / (2 * true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0,\n",
        "    \n",
        "    }\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"  False Positive Rate: {false_positive_rate:.4f} (expected: {error_rate})\")\n",
        "    print(f\"  False Negative Rate: {false_negative_rate:.4f}\")\n",
        "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
        "    print(f\"  Memory for model: {results['memory_for_model']:.2f} MB\")\n",
        "    print(f\"  Memory for Bloom filters: {results['memory_for_filters']:.2f} MB\")\n",
        "    print(f\"  Total memory (model + filters): {results['total_memory']:.2f} MB\")\n",
        "    print(f\"  Bits per item: {results['bits_per_item']:.2f}\")\n",
        "    print(f\"  Model creation time: {model_creation_time:.2f} seconds\")\n",
        "    print(f\"  Add time: {add_time:.2f} seconds\")\n",
        "    print(f\"  Test time: {results['test_time']:.2f} seconds\")\n",
        "    print(f\"  Adaptation metrics:\")\n",
        "    print(f\"    Initial partitions: {results['num_partitions_before']}\")\n",
        "    print(f\"    Final partitions: {results['num_partitions_after']}\")\n",
        "    print(f\"    Adaptation time: {adaptation_time:.2f} seconds\")\n",
        "    print(f\"    Average entropy before: {results['average_entropy_before']:.4f}\")\n",
        "    print(f\"    Average entropy after: {results['average_entropy_after']:.4f}\")\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_all_results(standard_results, plbf_results, clbf_results, dmlbf_results):\n",
        "    \"\"\"\n",
        "    Compare and print the results of all Bloom filter implementations.\n",
        "    \n",
        "    Args:\n",
        "        standard_results: Results from standard Bloom filter evaluation\n",
        "        plbf_results: Results from PLBF evaluation\n",
        "        clbf_results: Results from CLBF evaluation\n",
        "        dmlbf_results: Results from DML-LBF evaluation\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON OF ALL BLOOM FILTER IMPLEMENTATIONS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Compare false positive rates\n",
        "    print(\"\\nFalse Positive Rates:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_positive_rate']:.4f} (expected: {result['error_rate']})\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_positive_rate']:.4f} (expected: {result['error_rate']})\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_positive_rate']:.4f} (expected: {result['error_rate']})\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_positive_rate']:.4f} (expected: {result['error_rate']})\")\n",
        "\n",
        "    # Compare false negative rates \n",
        "    print(\"\\nFalse Negative Rates:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_negative_rate']:.4f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_negative_rate']:.4f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_negative_rate']:.4f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['false_negative_rate']:.4f}\")\n",
        "        \n",
        "    # Compare Precision values \n",
        "    print(\"\\nPrecision of different filters:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['precision']:.4f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['precision']:.4f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['precision']:.4f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['precision']:.4f}\")\n",
        "\n",
        "    # Compare recall values\n",
        "    print(\"\\nRecall of different filters:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['recall']:.4f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['recall']:.4f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['recall']:.4f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['recall']:.4f}\")\n",
        "\n",
        "    \n",
        "        \n",
        "    # Compare memory usage\n",
        "    print(\"\\nMemory Usage:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['filter_size_kb']:.2f} MB\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['memory_for_model']:.2f} MB (model) + {result['memory_for_filters']:.2f} MB (filters) = {result['total_memory']:.2f} MB (total)\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['memory_for_model']:.2f} MB (model) + {result['memory_for_filters']:.2f} MB (filters) = {result['total_memory']:.2f} MB (total)\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['memory_for_model']:.2f} MB (model) + {result['memory_for_filters']:.2f} MB (filters) = {result['total_memory']:.2f} MB (total)\")\n",
        "    \n",
        "    # Compare bits per item\n",
        "    print(\"\\nBits per Item:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['bits_per_item']:.2f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['bits_per_item']:.2f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['bits_per_item']:.2f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['bits_per_item']:.2f}\")\n",
        "\n",
        "    # Compare accuracies\n",
        "    print(\"\\nAccuracy of different bloom filters:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['accuracy']:.4f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['accuracy']:.4f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['accuracy']:.4f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['accuracy']:.4f}\")\n",
        "\n",
        "    # Compare F1 Scores\n",
        "    print(\"\\nF1 Scores of different bloom filters:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['f1_score']:.4f}\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['f1_score']:.4f}\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['f1_score']:.4f}\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['f1_score']:.4f}\")\n",
        "    \n",
        "    # Compare add times\n",
        "    print(\"\\nAdd Times:\")\n",
        "    for result in standard_results:\n",
        "        print(f\"  {result['filter_type']}: {result['add_time']:.2f} seconds\")\n",
        "    for result in plbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['add_time']:.2f} seconds (+ {result['model_creation_time']:.2f} seconds for model training)\")\n",
        "    for result in clbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['add_time']:.2f} seconds (+ {result['model_creation_time']:.2f} seconds for model training)\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}: {result['add_time']:.2f} seconds (+ {result['model_creation_time']:.2f} seconds for model training)\")\n",
        "\n",
        "    # Compare query times\n",
        "    # print(\"\\nQuery Times (for both true positives and false positives):\")\n",
        "    # for result in standard_results:\n",
        "    #     total_query_time = result['test_time_positives'] + result['test_time_negatives']\n",
        "    #     print(f\"  {result['filter_type']}: {total_query_time:.2f} seconds\")\n",
        "    # for result in plbf_results:\n",
        "    #     total_query_time = result['test_time_positives'] + result['test_time_negatives']\n",
        "    #     print(f\"  {result['filter_type']}: {total_query_time:.2f} seconds\")\n",
        "    # for result in clbf_results:\n",
        "    #     total_query_time = result['test_time_positives'] + result['test_time_negatives']\n",
        "    #     print(f\"  {result['filter_type']}: {total_query_time:.2f} seconds\")\n",
        "    # for result in dmlbf_results:\n",
        "    #     total_query_time = result['test_time_positives'] + result['test_time_negatives']\n",
        "    #     print(f\"  {result['filter_type']}: {total_query_time:.2f} seconds\")\n",
        "    \n",
        "    # Compare adaptation capabilities (only for DML-LBF)\n",
        "    print(\"\\nAdaptation Capabilities:\")\n",
        "    for result in dmlbf_results:\n",
        "        print(f\"  {result['filter_type']}:\")\n",
        "        print(f\"    Partitions before/after adaptation: {result['num_partitions_before']}/{result['num_partitions_after']}\")\n",
        "        print(f\"    Entropy before/after adaptation: {result['average_entropy_before']:.4f}/{result['average_entropy_after']:.4f}\")\n",
        "        print(f\"    Adaptation time: {result['adaptation_time']:.2f} seconds\")\n",
        "    \n",
        "    # Add summary section\n",
        "    print(\"\\nSUMMARY:\")\n",
        "    print(\"  Standard Bloom Filter advantages:\")\n",
        "    print(\"    - Simpler implementation\")\n",
        "    print(\"    - No training required\")\n",
        "    print(\"    - Faster setup time\")\n",
        "    print(\"    - Lower memory overhead for small datasets\")\n",
        "    \n",
        "    print(\"\\n  Partitioned Learned Bloom Filter advantages:\")\n",
        "    print(\"    - Can achieve lower false positive rates for the same memory usage\")\n",
        "    print(\"    - More efficient memory utilization for large datasets\")\n",
        "    print(\"    - Can be optimized for specific data distributions\")\n",
        "    print(\"    - Better scalability for very large datasets\")\n",
        "    \n",
        "    print(\"\\n  Cascaded Learned Bloom Filter advantages:\")\n",
        "    print(\"    - Fast rejection of obvious negatives in early stages\")\n",
        "    print(\"    - Optimal balance between model complexity and filter size\")\n",
        "    print(\"    - Progressive refinement through multiple stages\")\n",
        "    print(\"    - Can use different model complexities at different stages\")\n",
        "    \n",
        "    print(\"\\n  Dynamic Multi-Level Learned Bloom Filter advantages:\")\n",
        "    print(\"    - Entropy-aware resource allocation for optimal memory usage\")\n",
        "    print(\"    - Dynamic adaptation to changing data distributions\")\n",
        "    print(\"    - Automatic partition splitting and merging based on entropy and query patterns\")\n",
        "    print(\"    - Combines strengths of both partitioning and cascading approaches\")\n",
        "    print(\"    - Maintains performance stability under incremental workloads\")\n",
        "    \n",
        "    print(\"\\nRECOMMENDATION:\")\n",
        "    print(\"  - For small datasets or simple use cases: Standard Bloom Filter\")\n",
        "    print(\"  - For large datasets with uniform distribution: Partitioned Learned Bloom Filter\")\n",
        "    print(\"  - For datasets with varying difficulty of classification: Cascaded Learned Bloom Filter\")\n",
        "    print(\"  - For dynamic datasets with changing distributions: Dynamic Multi-Level Learned Bloom Filter\")\n",
        "    print(\"  - When both memory efficiency and adaptation are critical: Dynamic Multi-Level Learned Bloom Filter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(length_to_consider=10000, tts = 0.4):\n",
        "    \"\"\"Main function to run the comparison.\n",
        "\n",
        "    Args:\n",
        "        length_to_consider: Optional integer to limit the number of rows to use from the dataset.\n",
        "                           If None, the entire dataset is used.\n",
        "    \"\"\"\n",
        "    # Check if CUDA is available and print device information\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    if device.type == \"cuda\":\n",
        "        print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    print(\"Loading dataset...\")\n",
        "    df = pd.read_csv('/kaggle/input/binary-url-dataset-csv/binary_url_dataset.csv')\n",
        "    print(f\"Loaded {len(df)} rows from binary_url_dataset.csv\")\n",
        "\n",
        " \n",
        "    # Stratified sampling to maintain label distribution\n",
        "    \n",
        "\n",
        "    # Check for duplicates\n",
        "    print(\"Checking for duplicates...\")\n",
        "    duplicates = df.duplicated(subset=['url']).sum()\n",
        "    print(f\"Found {duplicates} duplicate URLs ({duplicates/len(df)*100:.2f}% of the dataset)\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['url'])\n",
        "    print(f\"Dataset after removing duplicates: {len(df)} rows\")\n",
        "       # Limit the dataset size if specified\n",
        "\n",
        "    print(f\"Limiting dataset to {length_to_consider} rows for faster processing\")\n",
        "    df = df[0:length_to_consider]\n",
        "    # Convert labels to binary (0 for safe, 1 for unsafe)\n",
        "    df['binary_label'] = df['label'].apply(lambda x: 1 if x == 'unsafe' else 0)\n",
        "    \n",
        "    safe_urls = df[df['binary_label'] == 0]\n",
        "    unsafe_urls = df[df['binary_label'] == 1]\n",
        "\n",
        "    print(f\"Safe URLs: {len(safe_urls)}\")\n",
        "    print(f\"Unsafe URLs: {len(unsafe_urls)}\")\n",
        "\n",
        "    # Train/test split within each group\n",
        "    safe_train_urls, safe_test_urls = train_test_split(\n",
        "        safe_urls, test_size=tts, random_state=42\n",
        "    )\n",
        "    unsafe_train_urls, unsafe_test_urls = train_test_split(\n",
        "        unsafe_urls, test_size=tts, random_state=42\n",
        "    )\n",
        "\n",
        "    # Print stats\n",
        "    print(f\"\\n--- Safe URLs ---\")\n",
        "    print(f\"Train: {len(safe_train_urls)}\")\n",
        "    print(f\"Test:  {len(safe_test_urls)}\")\n",
        "\n",
        "    print(f\"\\n--- Unsafe URLs ---\")\n",
        "    print(f\"Train: {len(unsafe_train_urls)}\")\n",
        "    print(f\"Test:  {len(unsafe_test_urls)}\")\n",
        "\n",
        "    # split the dataset into safe and unsafe URLs\n",
        "    safe_urls = df[df['binary_label'] == 0]\n",
        "    unsafe_urls = df[df['binary_label'] == 1]\n",
        "    print(f\"Safe URLs: {len(safe_urls)}\")\n",
        "    print(f\"Unsafe URLs: {len(unsafe_urls)}\")\n",
        "\n",
        "    \n",
        "    test_urls = pd.concat([unsafe_test_urls, safe_test_urls], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Evaluate standard Bloom filters with different error rates\n",
        "    capacity_standard = len(unsafe_urls)\n",
        "    standard_results = []\n",
        "    for error_rate in [0.1, 0.01, 0.001]:\n",
        "        result = evaluate_standard_bloom_filter(\n",
        "            unsafe_urls, test_urls, capacity=capacity_standard, error_rate=error_rate\n",
        "        )\n",
        "        standard_results.append(result)\n",
        "\n",
        "    # Evaluate PLBFs with different configurations\n",
        "    \n",
        "    plbf_results = []\n",
        "    train_plbf = pd.concat([unsafe_urls, safe_train_urls], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    capacity_plbf = len(unsafe_urls)\n",
        "    \n",
        "    train_urls_plbf = train_plbf['url'].values\n",
        "    train_labels_plbf = train_plbf['binary_label'].values\n",
        "    test_urls_plbf = test_urls['url'].values\n",
        "    test_labels_plbf = test_urls['binary_label'].values\n",
        "    \n",
        "    # Test with different error rates\n",
        "    for error_rate in [0.1, 0.01, 0.001]:\n",
        "        result = evaluate_partitioned_learned_bloom_filter(\n",
        "            train_urls_plbf, train_labels_plbf, test_urls_plbf, test_labels_plbf,\n",
        "            capacity=capacity_plbf, error_rate=error_rate, num_partitions=3\n",
        "        )\n",
        "        plbf_results.append(result)\n",
        "\n",
        "    # Evaluate CLBFs with different configurations\n",
        "    clbf_results = []\n",
        "\n",
        "    # Test with different error rates\n",
        "    for error_rate in [0.1, 0.01, 0.001]:\n",
        "        result = evaluate_cascaded_learned_bloom_filter(\n",
        "            train_urls_plbf, train_labels_plbf, test_urls_plbf, test_labels_plbf,\n",
        "            capacity=capacity_plbf, error_rate=error_rate, num_stages=2\n",
        "        )\n",
        "        clbf_results.append(result)\n",
        "        \n",
        "     # Evaluate DMLBFs with different configurations\n",
        "    dmlbf_results = []\n",
        "    \n",
        "    # Evaluate DML-LBF with different error rates\n",
        "    for error_rate in [0.1, 0.01, 0.001]:\n",
        "        result = evaluate_dynamic_multilevel_learned_bloom_filter(\n",
        "            train_urls_plbf, train_labels_plbf, test_urls_plbf, test_labels_plbf,\n",
        "            capacity=capacity_plbf, error_rate=error_rate, initial_partitions=8\n",
        "        )\n",
        "        dmlbf_results.append(result)\n",
        "\n",
        "    # Compare the results\n",
        "    compare_all_results(standard_results, plbf_results, clbf_results,dmlbf_results)\n",
        "\n",
        "    print(\"\\nComparison complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set the length_to_consider parameter here\n",
        "    # Use None to use the entire dataset, or set a specific number (e.g., 10000) for faster processing\n",
        "    length_to_consider = 10000  # Change this value as needed\n",
        "\n",
        "    # Run main function with specified length\n",
        "    main(length_to_consider=length_to_consider)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
